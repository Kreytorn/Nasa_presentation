{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive if on google collab\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = '/content/drive/MyDrive/nasa_exoplanet/Radial-Velocity'\n",
    "print(f\"Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Exoplanet RV: download + prep + balanced-train (RADIAL + HARPS) ===\n",
    "# - Downloads into /content/drive/MyDrive/nasa_exoplanet/\n",
    "# - Reads RADIAL .tbl with IPAC parser (fixes the tiny-positives issue)\n",
    "# - Builds tidy series: time, rv, rv_err, star_id, source, label\n",
    "# - Extracts per-star features (periodogram peaks + stats)\n",
    "# - Stratified 90/10 split by star\n",
    "# - Balanced training (undersample negatives to match positives) + LightGBM\n",
    "# - Saves artifacts to processed/\n",
    "\n",
    "import os, re, io, gzip, time, warnings, sys, subprocess\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "# deps\n",
    "try:\n",
    "    import requests, pandas as pd, numpy as np\n",
    "except:\n",
    "    pip_install([\"requests\",\"pandas\",\"numpy\"])\n",
    "    import requests, pandas as pd, numpy as np\n",
    "\n",
    "try:\n",
    "    from astropy.io import ascii as astro_ascii\n",
    "    from astropy.timeseries import LombScargle\n",
    "except:\n",
    "    pip_install([\"astropy\"])\n",
    "    from astropy.io import ascii as astro_ascii\n",
    "    from astropy.timeseries import LombScargle\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    pip_install([\"lightgbm\"])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, precision_recall_curve\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "BASE_DIR   = Path(\"/content/drive/MyDrive/nasa_exoplanet/Radial-Velocity\")\n",
    "RADIAL_DIR = BASE_DIR / \"RADIAL_raw\"\n",
    "HARPS_DIR  = BASE_DIR / \"HARPS_raw\"\n",
    "PROC_DIR   = BASE_DIR / \"processed\"\n",
    "for d in (PROC_DIR, RADIAL_DIR, HARPS_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NASA Exoplanet Archive endpoints\n",
    "WGET_RADIAL_URL = \"https://exoplanetarchive.ipac.caltech.edu/bulk_data_download/wget_RADIAL.bat\"\n",
    "CONFIRMED_HOSTS_CSV = (\n",
    "    \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?\"\n",
    "    \"query=select+distinct+hostname+from+pscomppars&format=csv\"\n",
    ")\n",
    "\n",
    "# HARPS rvbank mirrors (Trifonov+2020 SERVAL)\n",
    "HARPS_URLS = [\n",
    "    \"https://cdsarc.cds.unistra.fr/ftp/J/A+A/636/A74/rvbank.dat.gz\",\n",
    "    \"https://cdsarc.u-strasbg.fr/ftp/J/A+A/636/A74/rvbank.dat.gz\",\n",
    "    \"https://cdsarc.cds.unistra.fr/ftp/J/A+A/636/A74/rvbank.dat\",\n",
    "]\n",
    "\n",
    "# knobs\n",
    "MAX_WORKERS     = 12\n",
    "TIMEOUT         = 60\n",
    "RANDOM_STATE    = 42\n",
    "TEST_FRAC       = 0.10   # ~10% by star\n",
    "MIN_OBS         = 3      # keep short series to retain positives\n",
    "BALANCE_RATIO   = 1.0    # undersample neg:pos ≈ 1.0 => 1:1\n",
    "# =====================================================\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\":\"RV-Pipeline/3.0\"})\n",
    "    return s\n",
    "\n",
    "def stream_download(s, url, dest: Path):\n",
    "    tmp = dest.with_suffix(dest.suffix+\".part\")\n",
    "    with s.get(url, stream=True, timeout=TIMEOUT) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(tmp,\"wb\") as f:\n",
    "            for chunk in r.iter_content(1024*64):\n",
    "                if chunk: f.write(chunk)\n",
    "    tmp.replace(dest)\n",
    "\n",
    "# ---------------- 1) RADIAL (download + parse IPAC) ----------------\n",
    "def fetch_radial(s):\n",
    "    print(\"Fetching RADIAL wget script …\")\n",
    "    bat = s.get(WGET_RADIAL_URL, timeout=TIMEOUT).text\n",
    "    (RADIAL_DIR/\"wget_RADIAL.bat\").write_text(bat, encoding=\"utf-8\")\n",
    "\n",
    "    pairs = []\n",
    "    for line in bat.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.lower().startswith((\"rem\",\"::\")): continue\n",
    "        m = re.search(r'(https?://[^\\s\"\\']+)', line)\n",
    "        if not m: continue\n",
    "        url = m.group(1)\n",
    "        if not re.search(r'\\.(tbl|csv|fits)(\\.gz)?$', url, re.I):  # keep only data\n",
    "            continue\n",
    "        m2 = re.search(r'-O\\s+(\"?)([^\"\\s]+)\\1', line)\n",
    "        out_name = m2.group(2).strip() if m2 else None\n",
    "        pairs.append((url, out_name))\n",
    "    # dedupe\n",
    "    seen = {}\n",
    "    for u,n in pairs:\n",
    "        if u not in seen or (seen[u] is None and n): seen[u]=n\n",
    "    pairs = [(u, seen[u]) for u in seen]\n",
    "\n",
    "    tasks=[]\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        for url, out_name in pairs:\n",
    "            fname = out_name or url.rstrip(\"/\").split(\"/\")[-1].split(\"?\")[0]\n",
    "            dest  = RADIAL_DIR/fname\n",
    "            if dest.exists(): continue\n",
    "            tasks.append(ex.submit(stream_download, s, url, dest))\n",
    "        for i, fut in enumerate(as_completed(tasks),1):\n",
    "            try: fut.result()\n",
    "            except Exception: pass\n",
    "            if i%25==0: print(f\"  … {i}/{len(tasks)} RADIAL files\")\n",
    "    print(f\"RADIAL: have {len(list(RADIAL_DIR.glob('*.tbl')))} .tbl files.\")\n",
    "\n",
    "# robust IPAC column mapping\n",
    "def _map_ipac_cols(df):\n",
    "    cl = {c.lower().strip(): c for c in df.columns}\n",
    "    def pick(cands):\n",
    "        for k in cands:\n",
    "            if k in cl: return cl[k]\n",
    "        # fallback: contains\n",
    "        for k in cl:\n",
    "            if any(sub in k for sub in cands): return cl[k]\n",
    "        return None\n",
    "    time_col = pick(['bjd_tdb','bjd','time','jd','jd_utc','mjd'])\n",
    "    rv_col   = pick(['rv','radial_velocity','vrad','mnvel','vel','velocity','v_r'])\n",
    "    err_col  = pick(['rv_err','sigma_rv','e_rv','erv','rv_error','sig_rv','stdev','unc_rv'])\n",
    "    return time_col, rv_col, err_col\n",
    "\n",
    "def read_radial_file(fp: Path):\n",
    "    # IPAC reader first; fallback to whitespace\n",
    "    try:\n",
    "        tab = astro_ascii.read(str(fp), format='ipac', guess=True, fast_reader=False)\n",
    "        df = tab.to_pandas()\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(fp, delim_whitespace=True, comment=\"#\")\n",
    "        except Exception:\n",
    "            return None\n",
    "    if df.empty: return None\n",
    "    tcol, rcol, ecol = _map_ipac_cols(df)\n",
    "    if tcol is None or rcol is None:\n",
    "        return None\n",
    "    out = pd.DataFrame({\n",
    "        \"time\": pd.to_numeric(df[tcol], errors=\"coerce\"),\n",
    "        \"rv\":   pd.to_numeric(df[rcol], errors=\"coerce\"),\n",
    "    })\n",
    "    if ecol and ecol in df.columns:\n",
    "        out[\"rv_err\"] = pd.to_numeric(df[ecol], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"rv_err\"] = np.nan\n",
    "    out = out.dropna(subset=[\"time\",\"rv\"])\n",
    "    if out.empty: return None\n",
    "    # star_id from filename family (UID_xxx_RVC_###)\n",
    "    m = re.match(r'^(.*)_RVC_', fp.stem)\n",
    "    star_id = m.group(1) if m else fp.stem\n",
    "    out[\"star_id\"] = star_id\n",
    "    out[\"source\"]  = \"RADIAL\"\n",
    "    out[\"label\"]   = 1\n",
    "    return out\n",
    "\n",
    "def load_radial_ipac():\n",
    "    fps = list(RADIAL_DIR.glob(\"*.tbl\"))\n",
    "    if not fps: return pd.DataFrame(columns=[\"time\",\"rv\",\"rv_err\",\"star_id\",\"source\",\"label\"])\n",
    "    rows=[]\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(read_radial_file, fp): fp for fp in fps}\n",
    "        for fut in as_completed(futs):\n",
    "            df = fut.result()\n",
    "            if df is not None: rows.append(df)\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"time\",\"rv\",\"rv_err\",\"star_id\",\"source\",\"label\"])\n",
    "\n",
    "# ---------------- 2) HARPS (download + parse) ----------------\n",
    "def fetch_harps(s):\n",
    "    existing = list(HARPS_DIR.glob(\"rvbank.dat\")) + list(HARPS_DIR.glob(\"rvbank.dat.gz\"))\n",
    "    if existing:\n",
    "        print(\"HARPS: file present:\", existing[0].name)\n",
    "        return existing[0]\n",
    "    for url in HARPS_URLS:\n",
    "        try:\n",
    "            print(\"Trying HARPS:\", url)\n",
    "            dest = HARPS_DIR / url.split(\"/\")[-1]\n",
    "            stream_download(s, url, dest)\n",
    "            print(\"HARPS: downloaded\", dest.name)\n",
    "            return dest\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(\"HARPS: download failed.\")\n",
    "    return None\n",
    "\n",
    "def load_harps(rvbank_path: Path|None):\n",
    "    if rvbank_path is None:\n",
    "        return pd.DataFrame(columns=[\"time\",\"rv\",\"rv_err\",\"star_id\",\"source\"])\n",
    "    if rvbank_path.suffix==\".gz\":\n",
    "        with gzip.open(rvbank_path,\"rt\",encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "            text=f.read()\n",
    "    else:\n",
    "        text=rvbank_path.read_text(encoding=\"utf-8\",errors=\"replace\")\n",
    "    rows=[]\n",
    "    for ln in text.splitlines():\n",
    "        if not ln or ln.startswith(\"#\"): continue\n",
    "        parts = re.split(r\"\\s+\", ln.strip())\n",
    "        if len(parts)<4: continue\n",
    "        try:\n",
    "            t=float(parts[1]); rv=float(parts[2]); er=float(parts[3])\n",
    "        except:\n",
    "            continue\n",
    "        rows.append((t,rv,er,parts[0]))\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"time\",\"rv\",\"rv_err\",\"star_id\",\"source\"])\n",
    "    df=pd.DataFrame(rows,columns=[\"time\",\"rv\",\"rv_err\",\"star_id\"])\n",
    "    df[\"source\"]=\"HARPS\"\n",
    "    return df\n",
    "\n",
    "def fetch_confirmed_hosts(s):\n",
    "    try:\n",
    "        csv = s.get(CONFIRMED_HOSTS_CSV, timeout=TIMEOUT).text\n",
    "        df = pd.read_csv(io.StringIO(csv))\n",
    "        df[\"hostname_norm\"]=df[\"hostname\"].astype(str).str.strip().str.lower()\n",
    "        return set(df[\"hostname_norm\"].tolist())\n",
    "    except Exception:\n",
    "        print(\"WARN: failed to fetch confirmed host list; treating HARPS all as negatives.\")\n",
    "        return set()\n",
    "\n",
    "def label_harps(df_harps, confirmed_set):\n",
    "    if df_harps.empty: return df_harps\n",
    "    df=df_harps.copy()\n",
    "    df[\"label\"]=df[\"star_id\"].astype(str).str.strip().str.lower().isin(confirmed_set).astype(int)\n",
    "    return df\n",
    "\n",
    "# ---------------- 3) Cleaning + series sanity ----------------\n",
    "def clean_series(df, min_obs=MIN_OBS):\n",
    "    if df.empty: return df\n",
    "    keep=[\"time\",\"rv\",\"rv_err\",\"star_id\",\"source\",\"label\"]\n",
    "    for c in keep:\n",
    "        if c not in df.columns: df[c]=np.nan\n",
    "    df=df[keep].copy()\n",
    "    df[\"time\"]=pd.to_numeric(df[\"time\"], errors=\"coerce\")\n",
    "    df[\"rv\"]=pd.to_numeric(df[\"rv\"], errors=\"coerce\")\n",
    "    df[\"rv_err\"]=pd.to_numeric(df[\"rv_err\"], errors=\"coerce\")\n",
    "    df=df.dropna(subset=[\"time\",\"rv\"])\n",
    "    # require min_obs per star\n",
    "    counts=df.groupby(\"star_id\")[\"time\"].count()\n",
    "    good=counts[counts>=min_obs].index\n",
    "    return df[df.star_id.isin(good)]\n",
    "\n",
    "# ---------------- 4) Features ----------------\n",
    "def features_of_one(df):\n",
    "    t=df[\"time\"].values.astype(float)\n",
    "    y=df[\"rv\"].values.astype(float)\n",
    "    if len(t)<3 or np.std(y)==0: return None\n",
    "    # detrend-light: normalize\n",
    "    y=(y-np.median(y))/(np.std(y)+1e-9)\n",
    "\n",
    "    try:\n",
    "        freq,power=LombScargle(t,y).autopower()\n",
    "        if len(freq)==0: return None\n",
    "        idx=np.argsort(power)[-3:]  # top-3 peaks\n",
    "        topf=freq[idx]; topp=power[idx]\n",
    "        bestf=topf[-1]\n",
    "        period = 1.0/bestf if bestf>0 else np.nan\n",
    "        # sinusoid amplitude at bestf\n",
    "        phi=2*np.pi*bestf*t\n",
    "        A=np.vstack([np.sin(phi), np.cos(phi), np.ones_like(phi)]).T\n",
    "        coef, *_ = np.linalg.lstsq(A, y, rcond=None)\n",
    "        amp=float(np.sqrt(coef[0]**2+coef[1]**2))\n",
    "        p1=float(topp[-1])\n",
    "        p2=float(topp[-2]) if len(topp)>1 else 0.0\n",
    "        p3=float(topp[-3]) if len(topp)>2 else 0.0\n",
    "    except Exception:\n",
    "        period=np.nan; amp=0.0; p1=p2=p3=0.0\n",
    "\n",
    "    return {\n",
    "        \"period\": period,\n",
    "        \"power1\": p1,\n",
    "        \"power2\": p2,\n",
    "        \"power3\": p3,\n",
    "        \"amp\":    amp,\n",
    "        \"rms\":    float(np.std(y)),\n",
    "        \"mad\":    float(np.median(np.abs(y-np.median(y)))),\n",
    "        \"skew\":   float(pd.Series(y).skew()),\n",
    "        \"n_obs\":  int(len(y)),\n",
    "        \"span_days\": float(t.max()-t.min()),\n",
    "    }\n",
    "\n",
    "def build_feature_table(series_df):\n",
    "    feats=[]\n",
    "    for sid,g in series_df.groupby(\"star_id\", sort=False):\n",
    "        f=features_of_one(g)\n",
    "        if f is None: continue\n",
    "        f[\"star_id\"]=sid\n",
    "        f[\"label\"]=int(g[\"label\"].iloc[0])\n",
    "        feats.append(f)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# ---------------- 5) Split (stratified by star) + balance ----------------\n",
    "def stratified_group_split(feat_df, test_frac=TEST_FRAC, seed=RANDOM_STATE):\n",
    "    X = feat_df.drop(columns=[\"label\",\"star_id\"])\n",
    "    y = feat_df[\"label\"].values\n",
    "    groups = feat_df[\"star_id\"].values\n",
    "    n_splits = max(3, int(round(1/test_frac)))\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    train_idx, test_idx = next(sgkf.split(X, y, groups))\n",
    "    train_df = feat_df.iloc[train_idx].copy()\n",
    "    test_df  = feat_df.iloc[test_idx].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "def make_balanced_train(train_df, ratio=BALANCE_RATIO, seed=RANDOM_STATE):\n",
    "    pos = train_df[train_df.label==1]\n",
    "    neg = train_df[train_df.label==0]\n",
    "    if len(pos)==0: raise RuntimeError(\"No positives in training after split.\")\n",
    "    need_neg = int(max(1, round(len(pos)*ratio)))\n",
    "    neg_bal = neg.sample(n=min(need_neg, len(neg)), random_state=seed, replace=False)\n",
    "    bal = pd.concat([pos, neg_bal], ignore_index=True).sample(frac=1.0, random_state=seed)\n",
    "    return bal\n",
    "\n",
    "# ---------------- 6) Train + eval ----------------\n",
    "def train_lightgbm(train_df, test_df, seed=RANDOM_STATE):\n",
    "    features=[c for c in train_df.columns if c not in (\"star_id\",\"label\")]\n",
    "    Xtr, ytr = train_df[features].values, train_df[\"label\"].values\n",
    "    Xte, yte = test_df[features].values,  test_df[\"label\"].values\n",
    "\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=900, learning_rate=0.03, num_leaves=31,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        min_child_samples=20, random_state=seed\n",
    "    )\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    pred = clf.predict_proba(Xte)[:,1]\n",
    "    auc = roc_auc_score(yte, pred)\n",
    "    ap  = average_precision_score(yte, pred)\n",
    "    print(f\"\\n=== Eval ===\\nAUC: {auc:.3f} | AP: {ap:.3f}\")\n",
    "\n",
    "    # choose threshold by best F1 on PR curve\n",
    "    prec, rec, thr = precision_recall_curve(yte, pred)\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "    best_idx = int(np.argmax(f1))\n",
    "    best_thr = float(thr[best_idx]) if best_idx < len(thr) else 0.5\n",
    "    yhat = (pred>=best_thr).astype(int)\n",
    "    print(f\"Best F1 thr ~ {best_thr:.3f} | P={float(prec[best_idx]):.3f} R={float(rec[best_idx]):.3f}\")\n",
    "    print(\"\\nReport @bestF1:\\n\", classification_report(yte, yhat, digits=3))\n",
    "\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"gain\": clf.booster_.feature_importance(importance_type=\"gain\")\n",
    "    }).sort_values(\"gain\", ascending=False)\n",
    "    print(\"\\nTop features:\\n\", fi.head(10).to_string(index=False))\n",
    "    return clf, fi, pred, best_thr\n",
    "\n",
    "# ========================== RUN ==========================\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "with make_session() as s:\n",
    "    # download\n",
    "    fetch_radial(s)\n",
    "    harps_path = fetch_harps(s)\n",
    "    # load\n",
    "    df_radial = load_radial_ipac()\n",
    "    df_harps  = load_harps(harps_path)\n",
    "    # label harps via confirmed hosts\n",
    "    hosts = fetch_confirmed_hosts(s)\n",
    "    df_harps = label_harps(df_harps, hosts)\n",
    "\n",
    "# clean & keep short series too (>= MIN_OBS)\n",
    "df_radial = clean_series(df_radial, MIN_OBS)\n",
    "df_harps  = clean_series(df_harps,  MIN_OBS)\n",
    "\n",
    "# merge (RADIAL positives + HARPS mixed)\n",
    "df_all = pd.concat([df_radial, df_harps], ignore_index=True)\n",
    "\n",
    "# save series\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df_radial.to_parquet(PROC_DIR/\"radial_series.parquet\", index=False)\n",
    "if not df_harps.empty: df_harps.to_parquet(PROC_DIR/\"harps_series.parquet\", index=False)\n",
    "df_all.to_parquet(PROC_DIR/\"rv_series_merged.parquet\", index=False)\n",
    "\n",
    "# sanity log\n",
    "print(\"\\nSeries rows by source:\")\n",
    "print(df_all.groupby(\"source\").size())\n",
    "print(\"\\nStar counts by label:\")\n",
    "star_lab = df_all.groupby(\"star_id\")[\"label\"].first().value_counts()\n",
    "print(star_lab)\n",
    "\n",
    "# features\n",
    "feat_df = build_feature_table(df_all).dropna()\n",
    "feat_df.to_parquet(PROC_DIR/\"rv_features.parquet\", index=False)\n",
    "print(\"\\nFeature table shape:\", feat_df.shape)\n",
    "print(\"Pos/Neg in features:\", feat_df[\"label\"].value_counts().to_dict())\n",
    "\n",
    "# split (stratified by star) and balance train\n",
    "train_df, test_df = stratified_group_split(feat_df, TEST_FRAC, RANDOM_STATE)\n",
    "print(f\"\\nSplit → Train stars: {train_df['star_id'].nunique()} | Test stars: {test_df['star_id'].nunique()}\")\n",
    "print(\"Train pos/neg:\", train_df[\"label\"].sum(), \"/\", len(train_df)-train_df[\"label\"].sum())\n",
    "print(\"Test  pos/neg:\", test_df[\"label\"].sum(),  \"/\", len(test_df)-test_df[\"label\"].sum())\n",
    "\n",
    "train_bal = make_balanced_train(train_df, ratio=BALANCE_RATIO, seed=RANDOM_STATE)\n",
    "print(\"\\nBalanced train size:\", train_bal.shape, \"→ pos/neg:\",\n",
    "      int(train_bal['label'].sum()), \"/\", int(len(train_bal)-train_bal['label'].sum()))\n",
    "\n",
    "# train\n",
    "model, fi, pred, best_thr = train_lightgbm(train_bal, test_df)\n",
    "\n",
    "# save artifacts\n",
    "(model.booster_).save_model(str(PROC_DIR/\"lightgbm_rv.txt\"))\n",
    "fi.to_csv(PROC_DIR/\"feature_importances.csv\", index=False)\n",
    "out = test_df[[\"star_id\",\"label\"]].copy()\n",
    "out[\"pred_prob\"]=pred\n",
    "out[\"pred_label_bestF1\"]=(pred>=best_thr).astype(int)\n",
    "out.to_csv(PROC_DIR/\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Done. Artifacts:\")\n",
    "print(\"  -\", PROC_DIR/\"radial_series.parquet\")\n",
    "if not df_harps.empty: print(\"  -\", PROC_DIR/\"harps_series.parquet\")\n",
    "print(\"  -\", PROC_DIR/\"rv_series_merged.parquet\")\n",
    "print(\"  -\", PROC_DIR/\"rv_features.parquet\")\n",
    "print(\"  -\", PROC_DIR/\"feature_importances.csv\")\n",
    "print(\"  -\", PROC_DIR/\"lightgbm_rv.txt\")\n",
    "print(\"  -\", PROC_DIR/\"test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "PROC_DIR = Path(\"/content/drive/MyDrive/nasa_exoplanet/Radial-Velocity/processed\")\n",
    "\n",
    "series = pd.read_parquet(PROC_DIR/\"rv_series_merged.parquet\")\n",
    "print(series.groupby([\"source\"]).size().sort_values(ascending=False).head(10))\n",
    "print(series.groupby([\"source\",\"star_id\"]).size().describe())\n",
    "\n",
    "# After feature build:\n",
    "feat = pd.read_parquet(PROC_DIR/\"rv_features.parquet\")\n",
    "print(feat.groupby(\"label\").size())\n",
    "print(feat.groupby([\"label\"]).size(), \" total:\", len(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# rv_infer_standalone.py\n",
    "# Standalone inference helpers for your RV model.\n",
    "\n",
    "import sys, subprocess, io, re, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "def _pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    _pip_install([\"numpy\", \"pandas\"])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    _pip_install([\"lightgbm\"])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    from astropy.io import ascii as astro_ascii\n",
    "    from astropy.timeseries import LombScargle\n",
    "except Exception:\n",
    "    _pip_install([\"astropy\"])\n",
    "    from astropy.io import ascii as astro_ascii\n",
    "    from astropy.timeseries import LombScargle\n",
    "\n",
    "# ==== config: must match training features ====\n",
    "FEATURE_COLS = [\n",
    "    \"period\",\"power1\",\"power2\",\"power3\",\"amp\",\n",
    "    \"rms\",\"mad\",\"skew\",\"n_obs\",\"span_days\"\n",
    "]\n",
    "\n",
    "def load_model(model_path: str | Path) -> lgb.Booster:\n",
    "    model_path = Path(model_path)\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    return lgb.Booster(model_file=str(model_path))\n",
    "\n",
    "def _features_from_series(time: np.ndarray, rv: np.ndarray, rv_err: np.ndarray | None = None):\n",
    "    t = np.asarray(time, dtype=float)\n",
    "    y = np.asarray(rv, dtype=float)\n",
    "    if len(t) < 3 or np.std(y) == 0 or np.any(~np.isfinite(t)) or np.any(~np.isfinite(y)):\n",
    "        return None\n",
    "    y = (y - np.median(y)) / (np.std(y) + 1e-9)\n",
    "    try:\n",
    "        freq, power = LombScargle(t, y).autopower()\n",
    "    except Exception:\n",
    "        return None\n",
    "    if len(freq) == 0:\n",
    "        return None\n",
    "    idx = np.argsort(power)[-3:]\n",
    "    topf = freq[idx]; topp = power[idx]\n",
    "    bestf = topf[-1]\n",
    "    period = 1.0 / bestf if bestf > 0 else np.nan\n",
    "    phi = 2 * np.pi * bestf * t\n",
    "    A = np.vstack([np.sin(phi), np.cos(phi), np.ones_like(phi)]).T\n",
    "    coef, *_ = np.linalg.lstsq(A, y, rcond=None)\n",
    "    amp = float(np.sqrt(coef[0]**2 + coef[1]**2))\n",
    "    feats = {\n",
    "        \"period\": float(period),\n",
    "        \"power1\": float(topp[-1]),\n",
    "        \"power2\": float(topp[-2]) if len(topp) > 1 else 0.0,\n",
    "        \"power3\": float(topp[-3]) if len(topp) > 2 else 0.0,\n",
    "        \"amp\":    amp,\n",
    "        \"rms\":    float(np.std(y)),\n",
    "        \"mad\":    float(np.median(np.abs(y - np.median(y)))),\n",
    "        \"skew\":   float(pd.Series(y).skew()),\n",
    "        \"n_obs\":  int(len(y)),\n",
    "        \"span_days\": float(np.max(t) - np.min(t)),\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "def _map_cols(df: pd.DataFrame):\n",
    "    cl = {c.lower().strip(): c for c in df.columns}\n",
    "    def pick(cands):\n",
    "        for k in cands:\n",
    "            if k in cl: return cl[k]\n",
    "        for k in cl:\n",
    "            if any(sub in k for sub in cands): return cl[k]\n",
    "        return None\n",
    "    tcol = pick(['bjd_tdb','bjd','time','jd','jd_utc','mjd','date'])\n",
    "    rcol = pick(['rv','radial_velocity','vrad','mnvel','vel','velocity','v_r'])\n",
    "    ecol = pick(['rv_err','sigma_rv','e_rv','erv','rv_error','sig_rv','stdev','unc_rv'])\n",
    "    return tcol, rcol, ecol\n",
    "\n",
    "def _read_one_file(path: str | Path) -> tuple[pd.DataFrame, str]:\n",
    "    fp = Path(path)\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {fp}\")\n",
    "    data = None\n",
    "    name = fp.name.lower()\n",
    "    if name.endswith(\".tbl\"):\n",
    "        try:\n",
    "            tab = astro_ascii.read(str(fp), format=\"ipac\", guess=True, fast_reader=False)\n",
    "            data = tab.to_pandas()\n",
    "        except Exception:\n",
    "            data = None\n",
    "    if data is None:\n",
    "        try:\n",
    "            data = pd.read_csv(fp)\n",
    "        except Exception:\n",
    "            data = pd.read_csv(fp, delim_whitespace=True, comment=\"#\")\n",
    "    if data is None or data.empty:\n",
    "        raise ValueError(f\"Could not parse any rows from: {fp}\")\n",
    "    tcol, rcol, ecol = _map_cols(data)\n",
    "    if tcol is None or rcol is None:\n",
    "        raise ValueError(f\"Could not find time/rv columns in: {fp}\")\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": pd.to_numeric(data[tcol], errors=\"coerce\"),\n",
    "        \"rv\":   pd.to_numeric(data[rcol], errors=\"coerce\"),\n",
    "    })\n",
    "    if ecol and ecol in data.columns:\n",
    "        df[\"rv_err\"] = pd.to_numeric(data[ecol], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"rv_err\"] = np.nan\n",
    "    df = df.dropna(subset=[\"time\",\"rv\"]).sort_values(\"time\")\n",
    "    if len(df) < 3:\n",
    "        raise ValueError(f\"Need at least 3 valid (time, rv) points in: {fp}\")\n",
    "    m = re.match(r'^(.*)_RVC_', fp.stem, flags=re.IGNORECASE)\n",
    "    star_id = m.group(1) if m else fp.stem\n",
    "    return df, star_id\n",
    "\n",
    "def predict_from_arrays(model_path: str | Path,\n",
    "                        time, rv, rv_err=None,\n",
    "                        threshold: float = 0.5) -> dict:\n",
    "    booster = load_model(model_path)\n",
    "    time = np.asarray(time, dtype=float)\n",
    "    rv   = np.asarray(rv,   dtype=float)\n",
    "    if rv_err is None:\n",
    "        rv_err = np.full_like(rv, np.nan, dtype=float)\n",
    "    else:\n",
    "        rv_err = np.asarray(rv_err, dtype=float)\n",
    "    feats = _features_from_series(time, rv, rv_err)\n",
    "    if feats is None:\n",
    "        return {\"ok\": False, \"msg\": \"Not enough signal/points to compute features (need ≥3 and non-zero std).\"}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO2hYoE4X90tt+OFB5uC3Iw",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
