{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NASA Exoplanet Data Analysis and Processing\n",
        "\n",
        "This notebook merges NASA's three exoplanet datasets (Kepler, TESS, K2) to build a usable dataset for exoplanet classification.\n",
        "\n",
        "## Datasets:\n",
        "- **Kepler KOI Data**: Exoplanet candidates detected by the Kepler mission\n",
        "- **TESS TOI Data**: Exoplanet candidates detected by the TESS mission\n",
        "- **K2 Data**: Exoplanet candidates detected by the K2 mission\n",
        "\n",
        "## Goals:\n",
        "1. Merge the three datasets\n",
        "2. Identify common columns\n",
        "3. Improve data quality\n",
        "4. Produce a dataset ready for exoplanet classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kütüphaneler başarıyla yüklendi!\n",
            "Pandas version: 2.3.3\n",
            "NumPy version: 2.3.3\n",
            "Scikit-learn version: 1.7.2\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Model Training\n",
        "# In this section, we will train a classification model with the unified dataset and classify candidates\n",
        "\n",
        "print(\"=== CLASSIFICATION MODEL TRAINING ===\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading and analysis\n",
        "print(\"=== DATA LOADING AND ANALYSIS ===\")\n",
        "\n",
        "# Load unified dataset\n",
        "df = pd.read_csv('unified_dataset_final.csv')\n",
        "print(f\"Unified dataset shape: {df.shape}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"Label distribution percentage:\")\n",
        "print(df['label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Load training data (with labels)\n",
        "train_df = pd.read_csv('training_data.csv')\n",
        "print(f\"\\nTraining data shape: {train_df.shape}\")\n",
        "print(f\"Training label distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "# Load candidate data\n",
        "candidate_df = pd.read_csv('candidate_data.csv')\n",
        "print(f\"\\nCandidate data shape: {candidate_df.shape}\")\n",
        "print(f\"Candidate label distribution:\")\n",
        "print(candidate_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Feature engineering and selection\"\"\"\n",
        "    # Encode categorical variables\n",
        "    le = LabelEncoder()\n",
        "    if 'source_dataset' in df.columns:\n",
        "        df['source_dataset_encoded'] = le.fit_transform(df['source_dataset'].astype(str))\n",
        "    \n",
        "    # Select numeric features\n",
        "    numeric_features = [\n",
        "        'ra_deg', 'dec_deg', 'period_days', 't0_bjd', 'transit_depth_ppm', \n",
        "        'duration_hours', 'impact_param', 'ecc', 'snr', 'rp_re', 'teq_k', \n",
        "        'insolation', 'teff_k', 'logg_cgs', 'feh_dex', 'mass_solar', \n",
        "        'radius_solar', 'mag_kepler', 'num_transits', 'ror_ratio', 'dor_ratio',\n",
        "        'fp_flag_nt', 'fp_flag_ss', 'fp_flag_co', 'fp_flag_ec', 'mag_tess',\n",
        "        'stellar_pmra', 'stellar_pmdec', 'stellar_distance'\n",
        "    ]\n",
        "    \n",
        "    # Check available columns\n",
        "    available_features = [col for col in numeric_features if col in df.columns]\n",
        "    print(f\"Number of available features: {len(available_features)}\")\n",
        "    print(f\"Features: {available_features}\")\n",
        "    \n",
        "    # Add source dataset encoded\n",
        "    if 'source_dataset_encoded' in df.columns:\n",
        "        available_features.append('source_dataset_encoded')\n",
        "    \n",
        "    # Build feature matrix\n",
        "    X = df[available_features].copy()\n",
        "    \n",
        "    # Fill missing values\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
        "    \n",
        "    print(f\"Feature matrix shape: {X_imputed.shape}\")\n",
        "    print(f\"Missing value count: {X_imputed.isnull().sum().sum()}\")\n",
        "    \n",
        "    return X_imputed, available_features\n",
        "\n",
        "# Feature engineering for training data\n",
        "X_train, features = prepare_features(train_df)\n",
        "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "\n",
        "# Feature engineering for candidate data\n",
        "X_candidates, _ = prepare_features(candidate_df)\n",
        "print(f\"Candidate features shape: {X_candidates.shape}\")\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_df['label'].values\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Training label distribution: {np.unique(y_train, return_counts=True)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training\n",
        "print(\"\\n=== MODEL TRAINING ===\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "\n",
        "# Train-test split\n",
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train set shape: {X_train_split.shape}\")\n",
        "print(f\"Test set shape: {X_test_split.shape}\")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Compare model performance via cross-validation\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_split, y_train_split, cv=5, scoring='accuracy')\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std()\n",
        "    }\n",
        "    print(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Select the best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['cv_mean'])\n",
        "best_model = results[best_model_name]['model']\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name}\")\n",
        "print(f\"CV Accuracy: {results[best_model_name]['cv_mean']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "print(f\"\\n=== {best_model_name} MODEL EVALUATION ===\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model.fit(X_train_split, y_train_split)\n",
        "y_pred = best_model.predict(X_test_split)\n",
        "y_pred_proba = best_model.predict_proba(X_test_split)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test_split, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "if y_pred_proba is not None:\n",
        "    auc = roc_auc_score(y_test_split, y_pred_proba)\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_split, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_split, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrain on all training data and classify candidates\n",
        "print(f\"\\n=== RETRAIN ON FULL TRAINING DATA ===\")\n",
        "best_model.fit(X_train, y_train)\n",
        "print(\"Model retrained on full training data.\")\n",
        "\n",
        "print(\"\\n=== CANDIDATE CLASSIFICATION ===\")\n",
        "\n",
        "# Classify candidates\n",
        "candidate_predictions = best_model.predict(X_candidates)\n",
        "candidate_probabilities = best_model.predict_proba(X_candidates)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "# Append results to dataframe\n",
        "candidate_predictions_df = candidate_df.copy()\n",
        "candidate_predictions_df['predicted_label'] = candidate_predictions\n",
        "if candidate_probabilities is not None:\n",
        "    candidate_predictions_df['prediction_probability'] = candidate_probabilities\n",
        "\n",
        "# Prediction distribution\n",
        "print(f\"Candidate prediction distribution:\")\n",
        "print(pd.Series(candidate_predictions).value_counts())\n",
        "print(f\"Candidate prediction distribution (%):\")\n",
        "print(pd.Series(candidate_predictions).value_counts(normalize=True) * 100)\n",
        "\n",
        "print(f\"\\nTotal candidates: {len(candidate_predictions)}\")\n",
        "print(f\"Predicted as exoplanet: {sum(candidate_predictions == 1)}\")\n",
        "print(f\"Predicted as non-exoplanet: {sum(candidate_predictions == 0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Dataset Creation\n",
        "print(\"\\n=== FINAL DATASET CREATION ===\")\n",
        "\n",
        "# Add prediction columns to training data\n",
        "train_final = train_df.copy()\n",
        "train_final['predicted_label'] = train_final['label']  # Ground-truth labels\n",
        "train_final['prediction_probability'] = 1.0  # 100% confidence for ground-truth\n",
        "train_final['data_type'] = 'training'\n",
        "\n",
        "# Add data_type to candidate predictions\n",
        "candidate_final = candidate_predictions_df.copy()\n",
        "candidate_final['data_type'] = 'candidate'\n",
        "\n",
        "# Concatenate\n",
        "final_dataset = pd.concat([train_final, candidate_final], ignore_index=True)\n",
        "\n",
        "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
        "print(f\"Training rows: {len(train_final)}\")\n",
        "print(f\"Candidate rows: {len(candidate_final)}\")\n",
        "print(f\"Overall predicted label distribution:\")\n",
        "print(final_dataset['predicted_label'].value_counts())\n",
        "print(f\"Predicted label distribution (%):\")\n",
        "print(final_dataset['predicted_label'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results\n",
        "print(\"\\n=== SAVING RESULTS ===\")\n",
        "\n",
        "# Save final dataset\n",
        "final_dataset.to_csv('final_classified_dataset.csv', index=False)\n",
        "print(\"final_classified_dataset.csv saved - All data (training + candidate predictions)\")\n",
        "\n",
        "# Save candidate-only predictions\n",
        "candidate_predictions_df.to_csv('candidate_predictions.csv', index=False)\n",
        "print(\"candidate_predictions.csv saved - Candidate predictions only\")\n",
        "\n",
        "# Model performance summary\n",
        "print(f\"\\n=== MODEL PERFORMANCE SUMMARY ===\")\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "print(f\"Total training records: {len(train_df)}\")\n",
        "print(f\"Total candidates: {len(candidate_df)}\")\n",
        "print(f\"Final dataset size: {len(final_dataset)}\")\n",
        "\n",
        "# Feature importance (for Random Forest)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    print(f\"\\n=== FEATURE IMPORTANCE (Top 10) ===\")\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': features,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Veri setleri yükleniyor...\n",
            "Kepler veri seti yüklendi: (9564, 150)\n",
            "TESS veri seti yüklendi: (7703, 92)\n",
            "K2 veri seti yüklendi: (4004, 357)\n",
            "\n",
            "=== VERİ SETİ ÖZETİ ===\n",
            "Toplam satır sayısı: 21271\n",
            "Toplam sütun sayısı (ortalama): 200\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Kepler KOI Data\n",
        "kepler_df = pd.read_csv(\"Nasa-Exoplanet/kepler_koi_data_cleaned.csv\")\n",
        "print(f\"Kepler dataset loaded: {kepler_df.shape}\")\n",
        "\n",
        "# TESS TOI Data  \n",
        "tess_df = pd.read_csv(\"Nasa-Exoplanet/tess_toi_data_cleaned.csv\")\n",
        "print(f\"TESS dataset loaded: {tess_df.shape}\")\n",
        "\n",
        "# K2 Data\n",
        "k2_df = pd.read_csv(\"Nasa-Exoplanet/k2_data_cleaned.csv\")\n",
        "print(f\"K2 dataset loaded: {k2_df.shape}\")\n",
        "\n",
        "print(\"\\n=== DATASET SUMMARY ===\")\n",
        "print(f\"Total rows: {kepler_df.shape[0] + tess_df.shape[0] + k2_df.shape[0]}\")\n",
        "print(f\"Average number of columns: {(kepler_df.shape[1] + tess_df.shape[1] + k2_df.shape[1]) / 3:.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SÜTUN ANALİZİ ===\n",
            "Ortak sütun sayısı: 4\n",
            "Ortak sütunlar: ['dec', 'ra', 'sky_coord.dec', 'sky_coord.ra']\n",
            "\n",
            "Kepler-TESS ortak sütunlar: 4\n",
            "Kepler-K2 ortak sütunlar: 4\n",
            "TESS-K2 ortak sütunlar: 43\n",
            "\n",
            "Önemli sütunlar: 28\n",
            "Önemli sütunlar: ['dec', 'disposition', 'koi_disposition', 'pl_masse', 'pl_masseerr1', 'pl_masseerr2', 'pl_orbper', 'pl_orbpererr1', 'pl_orbpererr2', 'pl_rade', 'pl_radeerr1', 'pl_radeerr2', 'ra', 'sky_coord.dec', 'sky_coord.ra', 'st_logg', 'st_loggerr1', 'st_loggerr2', 'st_mass', 'st_masserr1', 'st_masserr2', 'st_rad', 'st_raderr1', 'st_raderr2', 'st_teff', 'st_tefferr1', 'st_tefferr2', 'tfopwg_disp']\n"
          ]
        }
      ],
      "source": [
        "# Analyze dataset columns\n",
        "print(\"=== COLUMN ANALYSIS ===\")\n",
        "\n",
        "# Get columns for each dataset\n",
        "kepler_cols = set(kepler_df.columns)\n",
        "tess_cols = set(tess_df.columns)\n",
        "k2_cols = set(k2_df.columns)\n",
        "\n",
        "# Find common columns\n",
        "common_cols = kepler_cols.intersection(tess_cols).intersection(k2_cols)\n",
        "print(f\"Number of common columns: {len(common_cols)}\")\n",
        "print(f\"Common columns: {sorted(common_cols)}\")\n",
        "\n",
        "# Common columns between pairs\n",
        "kepler_tess_common = kepler_cols.intersection(tess_cols)\n",
        "kepler_k2_common = kepler_cols.intersection(k2_cols)\n",
        "tess_k2_common = tess_cols.intersection(k2_cols)\n",
        "\n",
        "print(f\"\\nKepler-TESS common columns: {len(kepler_tess_common)}\")\n",
        "print(f\"Kepler-K2 common columns: {len(kepler_k2_common)}\")\n",
        "print(f\"TESS-K2 common columns: {len(tess_k2_common)}\")\n",
        "\n",
        "# Identify important columns\n",
        "important_cols = {\n",
        "    'ra', 'dec', 'sky_coord.ra', 'sky_coord.dec',  # Coordinates\n",
        "    'pl_orbper', 'pl_orbpererr1', 'pl_orbpererr2',  # Orbital period\n",
        "    'pl_rade', 'pl_radeerr1', 'pl_radeerr2',  # Planet radius\n",
        "    'pl_masse', 'pl_masseerr1', 'pl_masseerr2',  # Planet mass\n",
        "    'st_teff', 'st_tefferr1', 'st_tefferr2',  # Stellar temperature\n",
        "    'st_logg', 'st_loggerr1', 'st_loggerr2',  # Stellar logg\n",
        "    'st_rad', 'st_raderr1', 'st_raderr2',  # Stellar radius\n",
        "    'st_mass', 'st_masserr1', 'st_masserr2',  # Stellar mass\n",
        "    'disposition', 'koi_disposition', 'tfopwg_disp'  # Disposition\n",
        "}\n",
        "\n",
        "print(f\"\\nImportant columns: {len(important_cols)}\")\n",
        "print(f\"Important columns: {sorted(important_cols)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERİ SETİ İSTATİSTİKLERİ ===\n",
            "\n",
            "--- KEPLER VERİ SETİ ---\n",
            "Satır sayısı: 9564\n",
            "Sütun sayısı: 150\n",
            "Eksik değer yüzdesi: 18.88%\n",
            "\n",
            "--- TESS VERİ SETİ ---\n",
            "Satır sayısı: 7703\n",
            "Sütun sayısı: 92\n",
            "Eksik değer yüzdesi: 11.24%\n",
            "\n",
            "--- K2 VERİ SETİ ---\n",
            "Satır sayısı: 4004\n",
            "Sütun sayısı: 357\n",
            "Eksik değer yüzdesi: 38.13%\n",
            "\n",
            "=== DURUM BİLGİLERİ ===\n",
            "Kepler disposition dağılımı:\n",
            "koi_disposition\n",
            "FALSE POSITIVE    4839\n",
            "CONFIRMED         2746\n",
            "CANDIDATE         1979\n",
            "Name: count, dtype: int64\n",
            "\n",
            "TESS disposition dağılımı:\n",
            "tfopwg_disp\n",
            "PC     4679\n",
            "FP     1197\n",
            "CP      684\n",
            "KP      583\n",
            "APC     462\n",
            "FA       98\n",
            "Name: count, dtype: int64\n",
            "\n",
            "K2 disposition dağılımı:\n",
            "disposition\n",
            "CONFIRMED         2315\n",
            "CANDIDATE         1374\n",
            "FALSE POSITIVE     293\n",
            "REFUTED             22\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Show basic statistics of the datasets\n",
        "print(\"=== DATASET STATISTICS ===\")\n",
        "\n",
        "# Kepler dataset\n",
        "print(\"\\n--- KEPLER DATASET ---\")\n",
        "print(f\"Rows: {kepler_df.shape[0]}\")\n",
        "print(f\"Columns: {kepler_df.shape[1]}\")\n",
        "print(f\"Missing value percentage: {kepler_df.isnull().sum().sum() / (kepler_df.shape[0] * kepler_df.shape[1]) * 100:.2f}%\")\n",
        "\n",
        "# TESS dataset\n",
        "print(\"\\n--- TESS DATASET ---\")\n",
        "print(f\"Rows: {tess_df.shape[0]}\")\n",
        "print(f\"Columns: {tess_df.shape[1]}\")\n",
        "print(f\"Missing value percentage: {tess_df.isnull().sum().sum() / (tess_df.shape[0] * tess_df.shape[1]) * 100:.2f}%\")\n",
        "\n",
        "# K2 dataset\n",
        "print(\"\\n--- K2 DATASET ---\")\n",
        "print(f\"Rows: {k2_df.shape[0]}\")\n",
        "print(f\"Columns: {k2_df.shape[1]}\")\n",
        "print(f\"Missing value percentage: {k2_df.isnull().sum().sum() / (k2_df.shape[0] * k2_df.shape[1]) * 100:.2f}%\")\n",
        "\n",
        "# Check disposition columns\n",
        "print(\"\\n=== DISPOSITION INFORMATION ===\")\n",
        "if 'koi_disposition' in kepler_df.columns:\n",
        "    print(\"Kepler disposition distribution:\")\n",
        "    print(kepler_df['koi_disposition'].value_counts())\n",
        "    \n",
        "if 'tfopwg_disp' in tess_df.columns:\n",
        "    print(\"\\nTESS disposition distribution:\")\n",
        "    print(tess_df['tfopwg_disp'].value_counts())\n",
        "    \n",
        "if 'disposition' in k2_df.columns:\n",
        "    print(\"\\nK2 disposition distribution:\")\n",
        "    print(k2_df['disposition'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERİ BİRLEŞTİRME HAZIRLIĞI ===\n",
            "KAPSAMLI ortak özellikler:\n",
            "ra: ['ra', 'ra', 'ra']\n",
            "dec: ['dec', 'dec', 'dec']\n",
            "period_days: ['koi_period', 'pl_orbper', 'pl_orbper']\n",
            "eccentricity: ['koi_eccen', None, 'pl_orbeccen']\n",
            "inclination: ['koi_incl', None, 'pl_orbincl']\n",
            "semi_major_axis: ['koi_sma', None, 'pl_orbsmax']\n",
            "planet_radius_re: ['koi_prad', 'pl_rade', 'pl_rade']\n",
            "planet_mass_me: ['koi_smass', 'pl_masse', 'pl_masse']\n",
            "planet_density: [None, None, 'pl_dens']\n",
            "equilibrium_temp: ['koi_teq', 'pl_eqt', 'pl_eqt']\n",
            "insolation: ['koi_insol', 'pl_insol', 'pl_insol']\n",
            "transit_depth_ppm: ['koi_depth', 'pl_trandep', 'pl_trandep']\n",
            "transit_duration_hours: ['koi_duration', 'pl_trandurh', 'pl_trandur']\n",
            "transit_epoch: ['koi_time0bk', 'pl_tranmid', 'pl_tranmid']\n",
            "impact_parameter: ['koi_impact', None, 'pl_imppar']\n",
            "stellar_teff: ['koi_steff', 'st_teff', 'st_teff']\n",
            "stellar_logg: ['koi_slogg', 'st_logg', 'st_logg']\n",
            "stellar_radius: ['koi_srad', 'st_rad', 'st_rad']\n",
            "stellar_mass: ['koi_smass', 'st_mass', 'st_mass']\n",
            "stellar_metallicity: ['koi_smet', None, 'st_met']\n",
            "stellar_age: ['koi_sage', None, 'st_age']\n",
            "stellar_density: [None, None, 'st_dens']\n",
            "mag_kepler: ['koi_kepmag', None, 'sy_kepmag']\n",
            "mag_tess: [None, 'st_tmag', 'sy_tmag']\n",
            "mag_gaia_g: [None, None, 'sy_gaiamag']\n",
            "mag_bp: [None, None, 'sy_bmag']\n",
            "mag_rp: [None, None, 'sy_vmag']\n",
            "mag_j: ['koi_jmag', None, 'sy_jmag']\n",
            "mag_h: ['koi_hmag', None, 'sy_hmag']\n",
            "mag_k: ['koi_kmag', None, 'sy_kmag']\n",
            "rv_semi_amplitude: [None, None, 'pl_rvamp']\n",
            "num_transits: ['koi_num_transits', None, None]\n",
            "model_snr: ['koi_model_snr', None, None]\n",
            "ror_ratio: ['koi_ror', None, None]\n",
            "dor_ratio: ['koi_dor', None, None]\n",
            "fp_flag_nt: ['koi_fpflag_nt', None, None]\n",
            "fp_flag_ss: ['koi_fpflag_ss', None, None]\n",
            "fp_flag_co: ['koi_fpflag_co', None, None]\n",
            "fp_flag_ec: ['koi_fpflag_ec', None, None]\n",
            "stellar_pmra: [None, 'st_pmra', None]\n",
            "stellar_pmdec: [None, 'st_pmdec', None]\n",
            "stellar_distance: [None, 'st_dist', None]\n",
            "planet_radius_j: [None, None, 'pl_radj']\n",
            "planet_mass_j: [None, None, 'pl_massj']\n",
            "stellar_luminosity: [None, None, 'st_lum']\n",
            "stellar_vsini: [None, None, 'st_vsin']\n",
            "stellar_radv: [None, None, 'st_radv']\n",
            "disposition: ['koi_disposition', 'tfopwg_disp', 'disposition']\n",
            "\n",
            "Veri setleri source sütunu ile işaretlendi.\n",
            "Kepler: source_dataset\n",
            "Kepler    9564\n",
            "Name: count, dtype: int64\n",
            "TESS: source_dataset\n",
            "TESS    7703\n",
            "Name: count, dtype: int64\n",
            "K2: source_dataset\n",
            "K2    4004\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Determine common columns for merging datasets\n",
        "print(\"=== MERGE PREPARATION ===\")\n",
        "\n",
        "# COMPREHENSIVE feature mapping - common + dataset-specific features (45+)\n",
        "common_features = {\n",
        "    # Coordinates (present in all datasets)\n",
        "    'ra': ['ra', 'ra', 'ra'],\n",
        "    'dec': ['dec', 'dec', 'dec'],\n",
        "    \n",
        "    # Orbital properties\n",
        "    'period_days': ['koi_period', 'pl_orbper', 'pl_orbper'],\n",
        "    'eccentricity': ['koi_eccen', None, 'pl_orbeccen'],\n",
        "    'inclination': ['koi_incl', None, 'pl_orbincl'],\n",
        "    'semi_major_axis': ['koi_sma', None, 'pl_orbsmax'],\n",
        "    \n",
        "    # Planet properties\n",
        "    'planet_radius_re': ['koi_prad', 'pl_rade', 'pl_rade'],\n",
        "    'planet_mass_me': ['koi_smass', 'pl_masse', 'pl_masse'],\n",
        "    'planet_density': [None, None, 'pl_dens'],\n",
        "    'equilibrium_temp': ['koi_teq', 'pl_eqt', 'pl_eqt'],\n",
        "    'insolation': ['koi_insol', 'pl_insol', 'pl_insol'],\n",
        "    \n",
        "    # Transit properties\n",
        "    'transit_depth_ppm': ['koi_depth', 'pl_trandep', 'pl_trandep'],\n",
        "    'transit_duration_hours': ['koi_duration', 'pl_trandurh', 'pl_trandur'],\n",
        "    'transit_epoch': ['koi_time0bk', 'pl_tranmid', 'pl_tranmid'],\n",
        "    'impact_parameter': ['koi_impact', None, 'pl_imppar'],\n",
        "    \n",
        "    # Stellar properties\n",
        "    'stellar_teff': ['koi_steff', 'st_teff', 'st_teff'],\n",
        "    'stellar_logg': ['koi_slogg', 'st_logg', 'st_logg'],\n",
        "    'stellar_radius': ['koi_srad', 'st_rad', 'st_rad'],\n",
        "    'stellar_mass': ['koi_smass', 'st_mass', 'st_mass'],\n",
        "    'stellar_metallicity': ['koi_smet', None, 'st_met'],\n",
        "    'stellar_age': ['koi_sage', None, 'st_age'],\n",
        "    'stellar_density': [None, None, 'st_dens'],\n",
        "    \n",
        "    # Photometry\n",
        "    'mag_kepler': ['koi_kepmag', None, 'sy_kepmag'],\n",
        "    'mag_tess': [None, 'st_tmag', 'sy_tmag'],\n",
        "    'mag_gaia_g': [None, None, 'sy_gaiamag'],\n",
        "    'mag_bp': [None, None, 'sy_bmag'],\n",
        "    'mag_rp': [None, None, 'sy_vmag'],\n",
        "    'mag_j': ['koi_jmag', None, 'sy_jmag'],\n",
        "    'mag_h': ['koi_hmag', None, 'sy_hmag'],\n",
        "    'mag_k': ['koi_kmag', None, 'sy_kmag'],\n",
        "    \n",
        "    # RV properties\n",
        "    'rv_semi_amplitude': [None, None, 'pl_rvamp'],\n",
        "    \n",
        "    # KEPLER-SPECIFIC IMPORTANT FEATURES\n",
        "    'num_transits': ['koi_num_transits', None, None],\n",
        "    'model_snr': ['koi_model_snr', None, None],\n",
        "    'ror_ratio': ['koi_ror', None, None],\n",
        "    'dor_ratio': ['koi_dor', None, None],\n",
        "    'fp_flag_nt': ['koi_fpflag_nt', None, None],\n",
        "    'fp_flag_ss': ['koi_fpflag_ss', None, None],\n",
        "    'fp_flag_co': ['koi_fpflag_co', None, None],\n",
        "    'fp_flag_ec': ['koi_fpflag_ec', None, None],\n",
        "    \n",
        "    # TESS-SPECIFIC IMPORTANT FEATURES\n",
        "    'stellar_pmra': [None, 'st_pmra', None],\n",
        "    'stellar_pmdec': [None, 'st_pmdec', None],\n",
        "    'stellar_distance': [None, 'st_dist', None],\n",
        "    \n",
        "    # K2-SPECIFIC IMPORTANT FEATURES\n",
        "    'planet_radius_j': [None, None, 'pl_radj'],\n",
        "    'planet_mass_j': [None, None, 'pl_massj'],\n",
        "    'stellar_luminosity': [None, None, 'st_lum'],\n",
        "    'stellar_vsini': [None, None, 'st_vsin'],\n",
        "    'stellar_radv': [None, None, 'st_radv'],\n",
        "    \n",
        "    # Disposition\n",
        "    'disposition': ['koi_disposition', 'tfopwg_disp', 'disposition']\n",
        "}\n",
        "\n",
        "print(\"COMPREHENSIVE common features:\")\n",
        "for feature, cols in common_features.items():\n",
        "    print(f\"{feature}: {cols}\")\n",
        "\n",
        "# Add source column for each dataset\n",
        "kepler_df['source_dataset'] = 'Kepler'\n",
        "tess_df['source_dataset'] = 'TESS'\n",
        "k2_df['source_dataset'] = 'K2'\n",
        "\n",
        "print(f\"\\nDatasets tagged with source column.\")\n",
        "print(f\"Kepler: {kepler_df['source_dataset'].value_counts()}\")\n",
        "print(f\"TESS: {tess_df['source_dataset'].value_counts()}\")\n",
        "print(f\"K2: {k2_df['source_dataset'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROFESYONEL VERİ BİRLEŞTİRME HAZIRLIĞI ===\n",
            "✅ PROFESYONEL SCHEMA TANIMLANDI\n",
            "Schema features: 42\n",
            "Kepler mappings: 29\n",
            "TESS mappings: 19\n",
            "K2 mappings: 36\n",
            "\n",
            "Veri setleri source sütunu ile işaretlendi.\n",
            "Kepler: source_dataset\n",
            "kepler    9564\n",
            "Name: count, dtype: int64\n",
            "TESS: source_dataset\n",
            "tess    7703\n",
            "Name: count, dtype: int64\n",
            "K2: source_dataset\n",
            "k2    4004\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# PROFESSIONAL SCHEMA-FIRST APPROACH\n",
        "print(\"=== PROFESSIONAL MERGE PREPARATION ===\")\n",
        "\n",
        "# Unified Schema Definition (Schema-First Approach)\n",
        "unified_schema = {\n",
        "    # Identifiers\n",
        "    'star_id': {'dtype': 'object', 'unit': 'dimensionless', 'description': 'Host star identifier'},\n",
        "    'planet_id': {'dtype': 'object', 'unit': 'dimensionless', 'description': 'Planet/candidate identifier'},\n",
        "    'source_dataset': {'dtype': 'object', 'unit': 'dimensionless', 'description': 'Source dataset (kepler/tess/k2)'},\n",
        "    \n",
        "    # Sky position\n",
        "    'ra_deg': {'dtype': 'float64', 'unit': 'degrees', 'description': 'Right ascension'},\n",
        "    'dec_deg': {'dtype': 'float64', 'unit': 'degrees', 'description': 'Declination'},\n",
        "    \n",
        "    # Labels\n",
        "    'label': {'dtype': 'int8', 'unit': 'dimensionless', 'description': '1=confirmed planet, 0=false positive, -1=candidate'},\n",
        "    'is_candidate': {'dtype': 'int8', 'unit': 'dimensionless', 'description': '1 if candidate (uncertain label)'},\n",
        "    \n",
        "    # Orbital parameters\n",
        "    'period_days': {'dtype': 'float64', 'unit': 'days', 'description': 'Orbital period'},\n",
        "    't0_bjd': {'dtype': 'float64', 'unit': 'BJD', 'description': 'Transit epoch'},\n",
        "    'transit_depth_ppm': {'dtype': 'float64', 'unit': 'ppm', 'description': 'Transit depth'},\n",
        "    'duration_hours': {'dtype': 'float64', 'unit': 'hours', 'description': 'Transit duration'},\n",
        "    'impact_param': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Impact parameter'},\n",
        "    'ecc': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Orbital eccentricity'},\n",
        "    'snr': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Signal-to-noise ratio'},\n",
        "    \n",
        "    # Planet parameters\n",
        "    'rp_re': {'dtype': 'float64', 'unit': 'R_earth', 'description': 'Planet radius in Earth radii'},\n",
        "    'mp_mj': {'dtype': 'float64', 'unit': 'M_jupiter', 'description': 'Planet mass in Jupiter masses'},\n",
        "    'teq_k': {'dtype': 'float64', 'unit': 'Kelvin', 'description': 'Equilibrium temperature'},\n",
        "    'insolation': {'dtype': 'float64', 'unit': 'S_earth', 'description': 'Insolation flux'},\n",
        "    \n",
        "    # Host star parameters\n",
        "    'teff_k': {'dtype': 'float64', 'unit': 'Kelvin', 'description': 'Stellar effective temperature'},\n",
        "    'logg_cgs': {'dtype': 'float64', 'unit': 'log(cm/s^2)', 'description': 'Stellar surface gravity'},\n",
        "    'feh_dex': {'dtype': 'float64', 'unit': 'dex', 'description': 'Stellar metallicity'},\n",
        "    'mass_solar': {'dtype': 'float64', 'unit': 'M_sun', 'description': 'Stellar mass'},\n",
        "    'radius_solar': {'dtype': 'float64', 'unit': 'R_sun', 'description': 'Stellar radius'},\n",
        "    \n",
        "    # Photometry\n",
        "    'mag_kepler': {'dtype': 'float64', 'unit': 'mag', 'description': 'Kepler-band magnitude'},\n",
        "    'mag_tess': {'dtype': 'float64', 'unit': 'mag', 'description': 'TESS magnitude'},\n",
        "    'mag_gaia_g': {'dtype': 'float64', 'unit': 'mag', 'description': 'Gaia G-band magnitude'},\n",
        "    \n",
        "    # KEPLER ÖZEL FEATURELAR\n",
        "    'num_transits': {'dtype': 'int16', 'unit': 'count', 'description': 'Number of transits observed'},\n",
        "    'model_snr': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Model signal-to-noise ratio'},\n",
        "    'ror_ratio': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Planet-to-star radius ratio'},\n",
        "    'dor_ratio': {'dtype': 'float64', 'unit': 'dimensionless', 'description': 'Planet-to-star density ratio'},\n",
        "    'fp_flag_nt': {'dtype': 'int8', 'unit': 'flag', 'description': 'False positive flag: not transit-like'},\n",
        "    'fp_flag_ss': {'dtype': 'int8', 'unit': 'flag', 'description': 'False positive flag: stellar variability'},\n",
        "    'fp_flag_co': {'dtype': 'int8', 'unit': 'flag', 'description': 'False positive flag: centroid offset'},\n",
        "    'fp_flag_ec': {'dtype': 'int8', 'unit': 'flag', 'description': 'False positive flag: ephemeris match'},\n",
        "    \n",
        "    # TESS ÖZEL FEATURELAR\n",
        "    'stellar_pmra': {'dtype': 'float64', 'unit': 'mas/yr', 'description': 'Stellar proper motion in RA'},\n",
        "    'stellar_pmdec': {'dtype': 'float64', 'unit': 'mas/yr', 'description': 'Stellar proper motion in Dec'},\n",
        "    'stellar_distance': {'dtype': 'float64', 'unit': 'pc', 'description': 'Stellar distance'},\n",
        "    \n",
        "    # K2 ÖZEL FEATURELAR\n",
        "    'planet_radius_j': {'dtype': 'float64', 'unit': 'R_jupiter', 'description': 'Planet radius in Jupiter radii'},\n",
        "    'planet_mass_j': {'dtype': 'float64', 'unit': 'M_jupiter', 'description': 'Planet mass in Jupiter masses'},\n",
        "    'stellar_luminosity': {'dtype': 'float64', 'unit': 'L_sun', 'description': 'Stellar luminosity'},\n",
        "    'stellar_vsini': {'dtype': 'float64', 'unit': 'km/s', 'description': 'Stellar rotational velocity'},\n",
        "    'stellar_radv': {'dtype': 'float64', 'unit': 'km/s', 'description': 'Stellar radial velocity'},\n",
        "}\n",
        "\n",
        "# Rename mappings for each dataset\n",
        "rename_maps = {\n",
        "    'kepler': {\n",
        "        # Identifiers\n",
        "        'kepid': 'star_id',\n",
        "        'kepoi_name': 'planet_id',\n",
        "        \n",
        "        # Sky position\n",
        "        'ra': 'ra_deg',\n",
        "        'dec': 'dec_deg',\n",
        "        \n",
        "        # Disposition\n",
        "        'koi_disposition': '_disposition',\n",
        "        \n",
        "        # Orbital parameters\n",
        "        'koi_period': 'period_days',\n",
        "        'koi_time0bk': '_t0_offset',  # Will need adjustment\n",
        "        'koi_time0': 't0_bjd',\n",
        "        'koi_depth': 'transit_depth_ppm',\n",
        "        'koi_duration': 'duration_hours',\n",
        "        'koi_impact': 'impact_param',\n",
        "        'koi_eccen': 'ecc',\n",
        "        'koi_model_snr': 'snr',\n",
        "        \n",
        "        # Planet parameters\n",
        "        'koi_prad': 'rp_re',\n",
        "        'koi_teq': 'teq_k',\n",
        "        'koi_insol': 'insolation',\n",
        "        \n",
        "        # Stellar parameters\n",
        "        'koi_steff': 'teff_k',\n",
        "        'koi_slogg': 'logg_cgs',\n",
        "        'koi_smet': 'feh_dex',\n",
        "        'koi_smass': 'mass_solar',\n",
        "        'koi_srad': 'radius_solar',\n",
        "        \n",
        "        # Photometry\n",
        "        'koi_kepmag': 'mag_kepler',\n",
        "        \n",
        "        # Kepler özel\n",
        "        'koi_num_transits': 'num_transits',\n",
        "        'koi_ror': 'ror_ratio',\n",
        "        'koi_dor': 'dor_ratio',\n",
        "        'koi_fpflag_nt': 'fp_flag_nt',\n",
        "        'koi_fpflag_ss': 'fp_flag_ss',\n",
        "        'koi_fpflag_co': 'fp_flag_co',\n",
        "        'koi_fpflag_ec': 'fp_flag_ec',\n",
        "    },\n",
        "    'tess': {\n",
        "        # Identifiers\n",
        "        'tid': 'star_id',\n",
        "        'toi': 'planet_id',\n",
        "        \n",
        "        # Sky position\n",
        "        'ra': 'ra_deg',\n",
        "        'dec': 'dec_deg',\n",
        "        \n",
        "        # Disposition\n",
        "        'tfopwg_disp': '_disposition',\n",
        "        \n",
        "        # Orbital parameters\n",
        "        'pl_orbper': 'period_days',\n",
        "        'pl_tranmid': 't0_bjd',\n",
        "        'pl_trandep': 'transit_depth_ppm',\n",
        "        'pl_trandurh': 'duration_hours',\n",
        "        \n",
        "        # Planet parameters\n",
        "        'pl_rade': 'rp_re',\n",
        "        'pl_eqt': 'teq_k',\n",
        "        'pl_insol': 'insolation',\n",
        "        \n",
        "        # Stellar parameters\n",
        "        'st_teff': 'teff_k',\n",
        "        'st_logg': 'logg_cgs',\n",
        "        'st_rad': 'radius_solar',\n",
        "        'st_tmag': 'mag_tess',\n",
        "        \n",
        "        # TESS özel\n",
        "        'st_pmra': 'stellar_pmra',\n",
        "        'st_pmdec': 'stellar_pmdec',\n",
        "        'st_dist': 'stellar_distance',\n",
        "    },\n",
        "    'k2': {\n",
        "        # Identifiers\n",
        "        'epic_number': 'star_id',\n",
        "        'epic_hostname': 'star_id',\n",
        "        'pl_name': 'planet_id',\n",
        "        'k2_name': 'planet_id',\n",
        "        \n",
        "        # Sky position\n",
        "        'ra': 'ra_deg',\n",
        "        'dec': 'dec_deg',\n",
        "        \n",
        "        # Disposition\n",
        "        'disposition': '_disposition',\n",
        "        \n",
        "        # Orbital parameters\n",
        "        'pl_orbper': 'period_days',\n",
        "        'pl_tranmid': 't0_bjd',\n",
        "        'pl_trandep': '_transit_depth_percent',  # Will convert from % to ppm\n",
        "        'pl_trandur': 'duration_hours',\n",
        "        'pl_imppar': 'impact_param',\n",
        "        'pl_orbeccen': 'ecc',\n",
        "        \n",
        "        # Planet parameters\n",
        "        'pl_rade': 'rp_re',\n",
        "        'pl_bmassj': 'mp_mj',\n",
        "        'pl_massj': 'mp_mj',\n",
        "        'pl_eqt': 'teq_k',\n",
        "        'pl_insol': 'insolation',\n",
        "        \n",
        "        # Stellar parameters\n",
        "        'st_teff': 'teff_k',\n",
        "        'k2_teff': 'teff_k',\n",
        "        'st_logg': 'logg_cgs',\n",
        "        'k2_logg': 'logg_cgs',\n",
        "        'st_met': 'feh_dex',\n",
        "        'k2_metfe': 'feh_dex',\n",
        "        'st_mass': 'mass_solar',\n",
        "        'k2_mass': 'mass_solar',\n",
        "        'st_rad': 'radius_solar',\n",
        "        'k2_rad': 'radius_solar',\n",
        "        \n",
        "        # Photometry\n",
        "        'k2_kepmag': 'mag_kepler',\n",
        "        'sy_kepmag': 'mag_kepler',\n",
        "        'sy_tmag': 'mag_tess',\n",
        "        'sy_gaiamag': 'mag_gaia_g',\n",
        "        \n",
        "        # K2 özel\n",
        "        'pl_radj': 'planet_radius_j',\n",
        "        'pl_massj': 'planet_mass_j',\n",
        "        'st_lum': 'stellar_luminosity',\n",
        "        'st_vsin': 'stellar_vsini',\n",
        "        'st_radv': 'stellar_radv',\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"✅ PROFESSIONAL SCHEMA DEFINED\")\n",
        "print(f\"Schema features: {len(unified_schema)}\")\n",
        "print(f\"Kepler mappings: {len(rename_maps['kepler'])}\")\n",
        "print(f\"TESS mappings: {len(rename_maps['tess'])}\")\n",
        "print(f\"K2 mappings: {len(rename_maps['k2'])}\")\n",
        "\n",
        "# Add source column for each dataset\n",
        "kepler_df['source_dataset'] = 'kepler'\n",
        "tess_df['source_dataset'] = 'tess'\n",
        "k2_df['source_dataset'] = 'k2'\n",
        "\n",
        "print(f\"\\nDatasets tagged with source column.\")\n",
        "print(f\"Kepler: {kepler_df['source_dataset'].value_counts()}\")\n",
        "print(f\"TESS: {tess_df['source_dataset'].value_counts()}\")\n",
        "print(f\"K2: {k2_df['source_dataset'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BINARY CLASSIFICATION STRATEGY ===\n",
            "Training Data: Confirmed (1) + False Positive (0)\n",
            "Prediction Data: Candidates (-1) - will be predicted with XGBoost\n",
            "\n",
            "Disposition Mapping:\n",
            "CONFIRMED       -> label= 1, candidate=0 -> TRAINING\n",
            "FALSE POSITIVE  -> label= 0, candidate=0 -> TRAINING\n",
            "CANDIDATE       -> label=-1, candidate=1 -> PREDICTION\n",
            "PC              -> label=-1, candidate=1 -> PREDICTION\n",
            "FP              -> label= 0, candidate=0 -> TRAINING\n",
            "CP              -> label= 1, candidate=0 -> TRAINING\n",
            "APC             -> label=-1, candidate=1 -> PREDICTION\n",
            "KP              -> label=-1, candidate=1 -> PREDICTION\n",
            "EB              -> label= 0, candidate=0 -> TRAINING\n",
            "REFUTED         -> label= 0, candidate=0 -> TRAINING\n",
            "\n",
            "✅ Binary classification strategy ready!\n"
          ]
        }
      ],
      "source": [
        "# PROFESYONEL DISPOSITION MAPPING - BINARY CLASSIFICATION STRATEGY\n",
        "def map_disposition_to_label(disposition):\n",
        "    \"\"\"\n",
        "    Map disposition string to binary label for training.\n",
        "    \n",
        "    Strategy:\n",
        "    - 1 = confirmed planet (for training)\n",
        "    - 0 = false positive (for training) \n",
        "    - -1 = candidate (excluded from training, will be predicted later)\n",
        "    \n",
        "    Returns:\n",
        "        (label, is_candidate) tuple\n",
        "        label: 1=confirmed, 0=false positive, -1=candidate\n",
        "        is_candidate: 1 if candidate, 0 otherwise\n",
        "    \"\"\"\n",
        "    if pd.isna(disposition):\n",
        "        return (-1, 1)  # Unknown = candidate\n",
        "    \n",
        "    disp_upper = str(disposition).upper().strip()\n",
        "    \n",
        "    # Confirmed planets (TRAINING DATA)\n",
        "    if any(x in disp_upper for x in ['CONFIRMED', 'CP', 'CONFIRMED PLANET']):\n",
        "        return (1, 0)\n",
        "    \n",
        "    # False positives (TRAINING DATA)\n",
        "    if any(x in disp_upper for x in ['FALSE POSITIVE', 'FP', 'FA', 'FALSE ALARM', 'EB', 'REFUTED']):\n",
        "        return (0, 0)\n",
        "    \n",
        "    # Candidates (EXCLUDED FROM TRAINING - will be predicted)\n",
        "    if any(x in disp_upper for x in ['CANDIDATE', 'PC', 'APC', 'KP']):\n",
        "        return (-1, 1)\n",
        "    \n",
        "    # Default: treat as candidate\n",
        "    return (-1, 1)\n",
        "\n",
        "# Test disposition mapping\n",
        "print(\"=== BINARY CLASSIFICATION STRATEGY ===\")\n",
        "print(\"Training Data: Confirmed (1) + False Positive (0)\")\n",
        "print(\"Prediction Data: Candidates (-1) - will be predicted with XGBoost\")\n",
        "print(\"\\nDisposition Mapping:\")\n",
        "test_dispositions = ['CONFIRMED', 'FALSE POSITIVE', 'CANDIDATE', 'PC', 'FP', 'CP', 'APC', 'KP', 'EB', 'REFUTED']\n",
        "for disp in test_dispositions:\n",
        "    label, is_candidate = map_disposition_to_label(disp)\n",
        "    status = \"TRAINING\" if label in [0, 1] else \"PREDICTION\"\n",
        "    print(f\"{disp:15} -> label={label:2}, candidate={is_candidate} -> {status}\")\n",
        "\n",
        "print(\"\\n✅ Binary classification strategy ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SCHEMA MAPPING TEST ===\n",
            "\n",
            "Mapping schema for kepler...\n",
            "  Mapped 30 columns\n",
            "  Final shape: (9564, 30)\n",
            "  Label distribution: {0: 4839, 1: 2746, -1: 1979}\n",
            "\n",
            "Mapping schema for tess...\n",
            "  Mapped 21 columns\n",
            "  Final shape: (7703, 21)\n",
            "  Label distribution: {-1: 5724, 0: 1295, 1: 684}\n",
            "\n",
            "Mapping schema for k2...\n",
            "  Mapped 27 columns\n",
            "  Final shape: (4004, 27)\n",
            "  Label distribution: {1: 2315, -1: 1374, 0: 315}\n",
            "\n",
            "✅ Schema mapping tamamlandı!\n",
            "Kepler mapped: (9564, 30)\n",
            "TESS mapped: (7703, 21)\n",
            "K2 mapped: (4004, 27)\n"
          ]
        }
      ],
      "source": [
        "# PROFESSIONAL SCHEMA MAPPING FUNCTION\n",
        "def apply_schema_mapping(df, dataset_name):\n",
        "    \"\"\"Apply schema mapping to a single dataset.\"\"\"\n",
        "    print(f\"\\nMapping schema for {dataset_name}...\")\n",
        "    \n",
        "    rename_map = rename_maps[dataset_name]\n",
        "    \n",
        "    # Find which columns exist in the dataframe\n",
        "    existing_mappings = {}\n",
        "    for src_col, tgt_col in rename_map.items():\n",
        "        if src_col in df.columns:\n",
        "            existing_mappings[src_col] = tgt_col\n",
        "    \n",
        "    # Rename columns\n",
        "    df_mapped = df.rename(columns=existing_mappings)\n",
        "    \n",
        "    # Add source dataset identifier\n",
        "    df_mapped['source_dataset'] = dataset_name\n",
        "    \n",
        "    # Handle disposition -> label mapping\n",
        "    if '_disposition' in df_mapped.columns:\n",
        "        label_data = df_mapped['_disposition'].apply(map_disposition_to_label)\n",
        "        df_mapped['label'] = label_data.apply(lambda x: x[0])\n",
        "        df_mapped['is_candidate'] = label_data.apply(lambda x: x[1])\n",
        "        df_mapped = df_mapped.drop('_disposition', axis=1)\n",
        "    else:\n",
        "        df_mapped['label'] = -1\n",
        "        df_mapped['is_candidate'] = 0\n",
        "    \n",
        "    # Handle K2 transit depth conversion (% to ppm)\n",
        "    if dataset_name == 'k2' and '_transit_depth_percent' in df_mapped.columns:\n",
        "        df_mapped['transit_depth_ppm'] = df_mapped['_transit_depth_percent'] * 10000  # % to ppm\n",
        "        df_mapped = df_mapped.drop('_transit_depth_percent', axis=1)\n",
        "    \n",
        "    # Handle Kepler time offset (BJD - 2,454,833.0)\n",
        "    if dataset_name == 'kepler' and '_t0_offset' in df_mapped.columns:\n",
        "        if 't0_bjd' not in df_mapped.columns or df_mapped['t0_bjd'].isna().all():\n",
        "            df_mapped['t0_bjd'] = df_mapped['_t0_offset'] + 2454833.0\n",
        "        df_mapped = df_mapped.drop('_t0_offset', axis=1, errors='ignore')\n",
        "    \n",
        "    # Keep only columns that are in unified schema\n",
        "    schema_cols = list(unified_schema.keys())\n",
        "    existing_schema_cols = [col for col in schema_cols if col in df_mapped.columns]\n",
        "    df_mapped = df_mapped[existing_schema_cols]\n",
        "    \n",
        "    # Remove duplicate columns (if any)\n",
        "    df_mapped = df_mapped.loc[:, ~df_mapped.columns.duplicated()]\n",
        "    \n",
        "    print(f\"  Mapped {len(existing_schema_cols)} columns\")\n",
        "    print(f\"  Final shape: {df_mapped.shape}\")\n",
        "    print(f\"  Label distribution: {df_mapped['label'].value_counts().to_dict()}\")\n",
        "    \n",
        "    return df_mapped\n",
        "\n",
        "# Test schema mapping\n",
        "print(\"=== SCHEMA MAPPING TEST ===\")\n",
        "kepler_mapped = apply_schema_mapping(kepler_df, 'kepler')\n",
        "tess_mapped = apply_schema_mapping(tess_df, 'tess')\n",
        "k2_mapped = apply_schema_mapping(k2_df, 'k2')\n",
        "\n",
        "print(f\"\\n✅ Schema mapping completed!\")\n",
        "print(f\"Kepler mapped: {kepler_mapped.shape}\")\n",
        "print(f\"TESS mapped: {tess_mapped.shape}\")\n",
        "print(f\"K2 mapped: {k2_mapped.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROFESYONEL VERİ BİRLEŞTİRME ===\n",
            "\n",
            "Checking for column conflicts...\n",
            "kepler columns: ['star_id', 'planet_id', 'source_dataset', 'ra_deg', 'dec_deg', 'label', 'is_candidate', 'period_days', 't0_bjd', 'transit_depth_ppm', 'duration_hours', 'impact_param', 'ecc', 'snr', 'rp_re', 'teq_k', 'insolation', 'teff_k', 'logg_cgs', 'feh_dex', 'mass_solar', 'radius_solar', 'mag_kepler', 'num_transits', 'ror_ratio', 'dor_ratio', 'fp_flag_nt', 'fp_flag_ss', 'fp_flag_co', 'fp_flag_ec']\n",
            "tess columns: ['star_id', 'planet_id', 'source_dataset', 'ra_deg', 'dec_deg', 'label', 'is_candidate', 'period_days', 't0_bjd', 'transit_depth_ppm', 'duration_hours', 'rp_re', 'teq_k', 'insolation', 'teff_k', 'logg_cgs', 'radius_solar', 'mag_tess', 'stellar_pmra', 'stellar_pmdec', 'stellar_distance']\n",
            "k2 columns: ['star_id', 'planet_id', 'source_dataset', 'ra_deg', 'dec_deg', 'label', 'is_candidate', 'period_days', 't0_bjd', 'transit_depth_ppm', 'duration_hours', 'impact_param', 'ecc', 'rp_re', 'mp_mj', 'teq_k', 'insolation', 'teff_k', 'logg_cgs', 'feh_dex', 'mass_solar', 'radius_solar', 'mag_kepler', 'mag_tess', 'mag_gaia_g', 'planet_radius_j', 'planet_mass_j']\n",
            "\n",
            "Total unique columns across all datasets: 38\n",
            "\n",
            "Aligning column structure...\n",
            "Common columns: 17\n",
            "kepler after alignment: (9564, 30)\n",
            "tess after alignment: (7703, 21)\n",
            "k2 after alignment: (4004, 27)\n",
            "\n",
            "Concatenating datasets...\n",
            "\n",
            "=== AGGRESSIVE FEATURE CLEANING ===\n",
            "Original shape: (21271, 38)\n",
            "\n",
            "1. Removing columns with >80% missing values...\n",
            "Removing 4 columns with >80% missing\n",
            "\n",
            "2. Removing constant columns...\n",
            "Removing 0 constant columns\n",
            "\n",
            "3. Removing low variance columns...\n",
            "Removing 0 low variance columns\n",
            "\n",
            "4. Removing ID columns...\n",
            "Removing 1 ID columns\n",
            "\n",
            "5. Removing error/uncertainty columns...\n",
            "Removing 0 error columns\n",
            "\n",
            "✅ Feature cleaning complete!\n",
            "Final shape: (21271, 33)\n",
            "Removed 122 columns\n",
            "Remaining features: 33\n",
            "✓ Unified dataset: 21271 rows, 33 columns\n",
            "  Label distribution: {-1: 9077, 0: 6449, 1: 5745}\n",
            "  Candidates: is_candidate column not found - will be created during imputation\n",
            "=== OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY ===\n",
            "\n",
            "Missing values per column (top 10):\n",
            "                  count  percent\n",
            "stellar_distance  13783    64.80\n",
            "stellar_pmdec     13702    64.42\n",
            "stellar_pmra      13702    64.42\n",
            "ror_ratio         12070    56.74\n",
            "dor_ratio         12070    56.74\n",
            "snr               12070    56.74\n",
            "num_transits      11707    55.04\n",
            "fp_flag_ec        11707    55.04\n",
            "fp_flag_co        11707    55.04\n",
            "fp_flag_ss        11707    55.04\n",
            "\n",
            "  Numeric columns: 30\n",
            "  Categorical columns: 3\n",
            "\n",
            "=== OPTIMIZED KNN IMPUTATION STRATEGY ===\n",
            "\n",
            "1. PROGRESSIVE IMPUTATION\n",
            "Imputation order (least missing first):\n",
            "  ra_deg: 0.0% missing\n",
            "  dec_deg: 0.0% missing\n",
            "  t0_bjd: 0.3% missing\n",
            "  period_days: 0.7% missing\n",
            "  radius_solar: 4.7% missing\n",
            "  duration_hours: 5.7% missing\n",
            "  teff_k: 7.7% missing\n",
            "  rp_re: 8.0% missing\n",
            "  transit_depth_ppm: 10.7% missing\n",
            "  logg_cgs: 13.5% missing\n",
            "\n",
            "2. SMART KNN IMPUTATION\n",
            "Columns to impute: 27\n",
            "Columns with >70% missing (will be dropped): 0\n",
            "\n",
            "Imputing 27 numeric columns with KNN...\n",
            "✅ KNN imputation completed for 27 columns\n",
            "\n",
            "3. CATEGORICAL IMPUTATION\n",
            "  star_id: filled with 'EPIC 206103150'\n",
            "\n",
            "4. FINAL DATA TYPE CONVERSION\n",
            "\n",
            "5. FINAL QUALITY CHECK\n",
            "Final dataset shape: (21271, 33)\n",
            "Missing values: 0\n",
            "Missing percentage: 0.00%\n",
            "Infinite values: 0\n",
            "\n",
            "✅ Optimized KNN cross-dataset imputation completed!\n",
            "Strategy: Progressive imputation with smart KNN (k=5, distance weights)\n",
            "Benefits: Cross-dataset learning, better imputation quality, maintained data relationships\n"
          ]
        }
      ],
      "source": [
        "# PROFESSIONAL DATA MERGE AND FEATURE CLEANING\n",
        "print(\"=== PROFESSIONAL DATA MERGE ===\")\n",
        "\n",
        "# Check for column conflicts before concatenation\n",
        "print(\"\\nChecking for column conflicts...\")\n",
        "all_columns = set()\n",
        "for name, df in [('kepler', kepler_mapped), ('tess', tess_mapped), ('k2', k2_mapped)]:\n",
        "    print(f\"{name} columns: {df.columns.tolist()}\")\n",
        "    all_columns.update(df.columns)\n",
        "\n",
        "print(f\"\\nTotal unique columns across all datasets: {len(all_columns)}\")\n",
        "\n",
        "# Ensure all DataFrames have the same columns\n",
        "print(\"\\nAligning column structure...\")\n",
        "common_columns = set(kepler_mapped.columns).intersection(set(tess_mapped.columns)).intersection(set(k2_mapped.columns))\n",
        "print(f\"Common columns: {len(common_columns)}\")\n",
        "\n",
        "# Add missing columns with NaN values\n",
        "for df, name in [(kepler_mapped, 'kepler'), (tess_mapped, 'tess'), (k2_mapped, 'k2')]:\n",
        "    missing_cols = common_columns - set(df.columns)\n",
        "    for col in missing_cols:\n",
        "        df[col] = np.nan\n",
        "    print(f\"{name} after alignment: {df.shape}\")\n",
        "\n",
        "# Concatenate all datasets\n",
        "print(\"\\nConcatenating datasets...\")\n",
        "unified_df = pd.concat([kepler_mapped, tess_mapped, k2_mapped], ignore_index=True, sort=False)\n",
        "\n",
        "# AGGRESSIVE FEATURE CLEANING\n",
        "print(\"\\n=== AGGRESSIVE FEATURE CLEANING ===\")\n",
        "print(f\"Original shape: {unified_df.shape}\")\n",
        "\n",
        "# 1. Remove columns with >80% missing values\n",
        "print(\"\\n1. Removing columns with >80% missing values...\")\n",
        "missing_pct = (unified_df.isnull().sum() / len(unified_df)) * 100\n",
        "high_missing_cols = missing_pct[missing_pct > 80].index.tolist()\n",
        "print(f\"Removing {len(high_missing_cols)} columns with >80% missing\")\n",
        "unified_df = unified_df.drop(columns=high_missing_cols)\n",
        "\n",
        "# 2. Remove constant columns (single value)\n",
        "print(\"\\n2. Removing constant columns...\")\n",
        "constant_cols = []\n",
        "for col in unified_df.columns:\n",
        "    if unified_df[col].nunique() <= 1:\n",
        "        constant_cols.append(col)\n",
        "print(f\"Removing {len(constant_cols)} constant columns\")\n",
        "unified_df = unified_df.drop(columns=constant_cols)\n",
        "\n",
        "# 3. Remove low variance numeric columns\n",
        "print(\"\\n3. Removing low variance columns...\")\n",
        "low_var_cols = []\n",
        "for col in unified_df.select_dtypes(include=[np.number]).columns:\n",
        "    if col not in ['label', 'is_candidate'] and unified_df[col].std() < 0.01:\n",
        "        low_var_cols.append(col)\n",
        "print(f\"Removing {len(low_var_cols)} low variance columns\")\n",
        "unified_df = unified_df.drop(columns=low_var_cols)\n",
        "\n",
        "# 4. Remove ID columns (not useful for prediction)\n",
        "print(\"\\n4. Removing ID columns...\")\n",
        "id_cols = [col for col in unified_df.columns if any(x in col.lower() for x in ['id', 'name', 'kepid', 'tid', 'epic'])]\n",
        "id_cols = [col for col in id_cols if col not in ['star_id', 'planet_id']]  # Keep our unified IDs\n",
        "print(f\"Removing {len(id_cols)} ID columns\")\n",
        "unified_df = unified_df.drop(columns=id_cols)\n",
        "\n",
        "# 5. Remove error columns (usually not predictive)\n",
        "print(\"\\n5. Removing error/uncertainty columns...\")\n",
        "error_cols = [col for col in unified_df.columns if any(x in col.lower() for x in ['err', 'error', 'uncertainty', 'flag'])]\n",
        "error_cols = [col for col in error_cols if col not in ['fp_flag_nt', 'fp_flag_ss', 'fp_flag_co', 'fp_flag_ec']]  # Keep false positive flags\n",
        "print(f\"Removing {len(error_cols)} error columns\")\n",
        "unified_df = unified_df.drop(columns=error_cols)\n",
        "\n",
        "print(f\"\\n✅ Feature cleaning complete!\")\n",
        "print(f\"Final shape: {unified_df.shape}\")\n",
        "print(f\"Removed {155 - unified_df.shape[1]} columns\")\n",
        "print(f\"Remaining features: {unified_df.shape[1]}\")\n",
        "\n",
        "print(f\"✓ Unified dataset: {unified_df.shape[0]} rows, {unified_df.shape[1]} columns\")\n",
        "print(f\"  Label distribution: {unified_df['label'].value_counts().to_dict()}\")\n",
        "if \"is_candidate\" in unified_df.columns:\n",
        "    print(f\"  Candidates: {unified_df['is_candidate'].sum()}\")\n",
        "else:\n",
        "    print(\"  Candidates: is_candidate column not found - will be created during imputation\")\n",
        "\n",
        "# OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY\n",
        "print(\"=== OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY ===\")\n",
        "\n",
        "# Report missing values before cleaning\n",
        "print(\"\\nMissing values per column (top 10):\")\n",
        "missing = unified_df.isnull().sum().sort_values(ascending=False)\n",
        "missing_pct = (missing / len(unified_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'count': missing, 'percent': missing_pct}).head(10)\n",
        "print(missing_df.to_string())\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "for col in unified_df.columns:\n",
        "    if col in ['star_id', 'planet_id', 'source_dataset']:\n",
        "        categorical_cols.append(col)\n",
        "    elif unified_df[col].dtype in ['float64', 'int64', 'int8', 'int16']:\n",
        "        numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\n  Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"  Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "# OPTIMIZED KNN IMPUTATION STRATEGY\n",
        "print(\"\\n=== OPTIMIZED KNN IMPUTATION STRATEGY ===\")\n",
        "\n",
        "# 1. PROGRESSIVE IMPUTATION (Aşamalı doldurma)\n",
        "print(\"\\n1. PROGRESSIVE IMPUTATION\")\n",
        "# Group columns by missing percentage\n",
        "missing_pct_numeric = {}\n",
        "for col in numeric_cols:\n",
        "    if col not in ['label', 'is_candidate']:\n",
        "        missing_pct_numeric[col] = (unified_df[col].isnull().sum() / len(unified_df)) * 100\n",
        "\n",
        "# Sort by missing percentage (least missing first)\n",
        "sorted_cols = sorted(missing_pct_numeric.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"Imputation order (least missing first):\")\n",
        "for col, pct in sorted_cols[:10]:\n",
        "    print(f\"  {col}: {pct:.1f}% missing\")\n",
        "\n",
        "# 2. SMART KNN IMPUTATION\n",
        "print(\"\\n2. SMART KNN IMPUTATION\")\n",
        "# Only impute columns with reasonable missing percentage\n",
        "numeric_to_impute = [col for col, pct in missing_pct_numeric.items() \n",
        "                     if pct < 70 and pct > 0]  # Between 0% and 70% missing\n",
        "\n",
        "print(f\"Columns to impute: {len(numeric_to_impute)}\")\n",
        "print(f\"Columns with >70% missing (will be dropped): {len([col for col, pct in missing_pct_numeric.items() if pct >= 70])}\")\n",
        "\n",
        "if numeric_to_impute:\n",
        "    print(f\"\\nImputing {len(numeric_to_impute)} numeric columns with KNN...\")\n",
        "    \n",
        "    # Prepare data for imputation\n",
        "    imputation_data = unified_df[numeric_to_impute].copy()\n",
        "    \n",
        "    # Handle infinite values\n",
        "    imputation_data = imputation_data.replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    # KNN Imputation with optimized parameters\n",
        "    imputer = KNNImputer(\n",
        "        n_neighbors=5, \n",
        "        weights='distance',\n",
        "        metric='nan_euclidean'\n",
        "    )\n",
        "    \n",
        "    # Fit and transform\n",
        "    imputed_data = imputer.fit_transform(imputation_data)\n",
        "    unified_df[numeric_to_impute] = imputed_data\n",
        "    \n",
        "    print(f\"✅ KNN imputation completed for {len(numeric_to_impute)} columns\")\n",
        "\n",
        "# 3. CATEGORICAL IMPUTATION\n",
        "print(\"\\n3. CATEGORICAL IMPUTATION\")\n",
        "for col in categorical_cols:\n",
        "    if unified_df[col].isna().any():\n",
        "        mode_val = unified_df[col].mode()[0] if len(unified_df[col].mode()) > 0 else 'unknown'\n",
        "        unified_df[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"  {col}: filled with '{mode_val}'\")\n",
        "\n",
        "# 4. FINAL DATA TYPE CONVERSION\n",
        "print(\"\\n4. FINAL DATA TYPE CONVERSION\")\n",
        "for col in unified_df.columns:\n",
        "    if col in unified_schema:\n",
        "        target_dtype = unified_schema[col]['dtype']\n",
        "        try:\n",
        "            if target_dtype == 'object':\n",
        "                unified_df[col] = unified_df[col].astype(str)\n",
        "            else:\n",
        "                unified_df[col] = unified_df[col].astype(target_dtype)\n",
        "        except:\n",
        "            print(f\"  Warning: Could not convert {col} to {target_dtype}\")\n",
        "\n",
        "# 5. FINAL QUALITY CHECK\n",
        "print(\"\\n5. FINAL QUALITY CHECK\")\n",
        "print(f\"Final dataset shape: {unified_df.shape}\")\n",
        "print(f\"Missing values: {unified_df.isnull().sum().sum()}\")\n",
        "print(f\"Missing percentage: {(unified_df.isnull().sum().sum() / (unified_df.shape[0] * unified_df.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "# Check for infinite values\n",
        "inf_count = np.isinf(unified_df.select_dtypes(include=[np.number])).sum().sum()\n",
        "print(f\"Infinite values: {inf_count}\")\n",
        "\n",
        "print(\"\\n✅ Optimized KNN cross-dataset imputation completed!\")\n",
        "print(\"Strategy: Progressive imputation with smart KNN (k=5, distance weights)\")\n",
        "print(\"Benefits: Cross-dataset learning, better imputation quality, maintained data relationships\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY ===\n",
            "\n",
            "Missing values per column (top 10):\n",
            "               count  percent\n",
            "star_id            0      0.0\n",
            "logg_cgs           0      0.0\n",
            "stellar_pmdec      0      0.0\n",
            "stellar_pmra       0      0.0\n",
            "mag_tess           0      0.0\n",
            "fp_flag_ec         0      0.0\n",
            "fp_flag_co         0      0.0\n",
            "fp_flag_ss         0      0.0\n",
            "fp_flag_nt         0      0.0\n",
            "dor_ratio          0      0.0\n",
            "\n",
            "  Numeric columns: 30\n",
            "  Categorical columns: 3\n",
            "\n",
            "=== OPTIMIZED KNN IMPUTATION STRATEGY ===\n",
            "\n",
            "1. PROGRESSIVE IMPUTATION\n",
            "Imputation order (least missing first):\n",
            "  ra_deg: 0.0% missing\n",
            "  dec_deg: 0.0% missing\n",
            "  period_days: 0.0% missing\n",
            "  t0_bjd: 0.0% missing\n",
            "  transit_depth_ppm: 0.0% missing\n",
            "  duration_hours: 0.0% missing\n",
            "  impact_param: 0.0% missing\n",
            "  ecc: 0.0% missing\n",
            "  snr: 0.0% missing\n",
            "  rp_re: 0.0% missing\n",
            "\n",
            "2. SMART KNN IMPUTATION\n",
            "Columns to impute: 0\n",
            "Columns with >70% missing (will be dropped): 0\n",
            "\n",
            "3. CATEGORICAL IMPUTATION\n",
            "\n",
            "4. FINAL DATA TYPE CONVERSION\n",
            "\n",
            "5. FINAL QUALITY CHECK\n",
            "Final dataset shape: (21271, 33)\n",
            "Missing values: 0\n",
            "Missing percentage: 0.00%\n",
            "Infinite values: 0\n",
            "\n",
            "✅ Optimized KNN cross-dataset imputation completed!\n",
            "Strategy: Progressive imputation with smart KNN (k=5, distance weights)\n",
            "Benefits: Cross-dataset learning, better imputation quality, maintained data relationships\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY\n",
        "print(\"=== OPTIMIZED KNN CROSS-DATASET IMPUTATION STRATEGY ===\")\n",
        "\n",
        "# Report missing values before cleaning\n",
        "print(\"\\nMissing values per column (top 10):\")\n",
        "missing = unified_df.isnull().sum().sort_values(ascending=False)\n",
        "missing_pct = (missing / len(unified_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'count': missing, 'percent': missing_pct}).head(10)\n",
        "print(missing_df.to_string())\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols = []\n",
        "categorical_cols = []\n",
        "\n",
        "for col in unified_df.columns:\n",
        "    if col in ['star_id', 'planet_id', 'source_dataset']:\n",
        "        categorical_cols.append(col)\n",
        "    elif unified_df[col].dtype in ['float64', 'int64', 'int8', 'int16']:\n",
        "        numeric_cols.append(col)\n",
        "\n",
        "print(f\"\\n  Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"  Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "# OPTIMIZED KNN IMPUTATION STRATEGY\n",
        "print(\"\\n=== OPTIMIZED KNN IMPUTATION STRATEGY ===\")\n",
        "\n",
        "# 1. PROGRESSIVE IMPUTATION (Aşamalı doldurma)\n",
        "print(\"\\n1. PROGRESSIVE IMPUTATION\")\n",
        "# Group columns by missing percentage\n",
        "missing_pct_numeric = {}\n",
        "for col in numeric_cols:\n",
        "    if col not in ['label', 'is_candidate']:\n",
        "        missing_pct_numeric[col] = (unified_df[col].isnull().sum() / len(unified_df)) * 100\n",
        "\n",
        "# Sort by missing percentage (least missing first)\n",
        "sorted_cols = sorted(missing_pct_numeric.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"Imputation order (least missing first):\")\n",
        "for col, pct in sorted_cols[:10]:\n",
        "    print(f\"  {col}: {pct:.1f}% missing\")\n",
        "\n",
        "# 2. SMART KNN IMPUTATION\n",
        "print(\"\\n2. SMART KNN IMPUTATION\")\n",
        "# Only impute columns with reasonable missing percentage\n",
        "numeric_to_impute = [col for col, pct in missing_pct_numeric.items() \n",
        "                     if pct < 70 and pct > 0]  # Between 0% and 70% missing\n",
        "\n",
        "print(f\"Columns to impute: {len(numeric_to_impute)}\")\n",
        "print(f\"Columns with >70% missing (will be dropped): {len([col for col, pct in missing_pct_numeric.items() if pct >= 70])}\")\n",
        "\n",
        "if numeric_to_impute:\n",
        "    print(f\"\\nImputing {len(numeric_to_impute)} numeric columns with KNN...\")\n",
        "    \n",
        "    # Prepare data for imputation\n",
        "    imputation_data = unified_df[numeric_to_impute].copy()\n",
        "    \n",
        "    # Handle infinite values\n",
        "    imputation_data = imputation_data.replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    # KNN Imputation with optimized parameters\n",
        "    imputer = KNNImputer(\n",
        "        n_neighbors=5, \n",
        "        weights='distance',\n",
        "        metric='nan_euclidean'\n",
        "    )\n",
        "    \n",
        "    # Fit and transform\n",
        "    imputed_data = imputer.fit_transform(imputation_data)\n",
        "    unified_df[numeric_to_impute] = imputed_data\n",
        "    \n",
        "    print(f\"✅ KNN imputation completed for {len(numeric_to_impute)} columns\")\n",
        "\n",
        "# 3. CATEGORICAL IMPUTATION\n",
        "print(\"\\n3. CATEGORICAL IMPUTATION\")\n",
        "for col in categorical_cols:\n",
        "    if unified_df[col].isna().any():\n",
        "        mode_val = unified_df[col].mode()[0] if len(unified_df[col].mode()) > 0 else 'unknown'\n",
        "        unified_df[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"  {col}: filled with '{mode_val}'\")\n",
        "\n",
        "# 4. FINAL DATA TYPE CONVERSION\n",
        "print(\"\\n4. FINAL DATA TYPE CONVERSION\")\n",
        "for col in unified_df.columns:\n",
        "    if col in unified_schema:\n",
        "        target_dtype = unified_schema[col]['dtype']\n",
        "        try:\n",
        "            if target_dtype == 'object':\n",
        "                unified_df[col] = unified_df[col].astype(str)\n",
        "            else:\n",
        "                unified_df[col] = unified_df[col].astype(target_dtype)\n",
        "        except:\n",
        "            print(f\"  Warning: Could not convert {col} to {target_dtype}\")\n",
        "\n",
        "# 5. FINAL QUALITY CHECK\n",
        "print(\"\\n5. FINAL QUALITY CHECK\")\n",
        "print(f\"Final dataset shape: {unified_df.shape}\")\n",
        "print(f\"Missing values: {unified_df.isnull().sum().sum()}\")\n",
        "print(f\"Missing percentage: {(unified_df.isnull().sum().sum() / (unified_df.shape[0] * unified_df.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "# Check for infinite values\n",
        "inf_count = np.isinf(unified_df.select_dtypes(include=[np.number])).sum().sum()\n",
        "print(f\"Infinite values: {inf_count}\")\n",
        "\n",
        "print(\"\\n✅ Optimized KNN cross-dataset imputation completed!\")\n",
        "print(\"Strategy: Progressive imputation with smart KNN (k=5, distance weights)\")\n",
        "print(\"Benefits: Cross-dataset learning, better imputation quality, maintained data relationships\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROFESYONEL ARTIFACT EXPORT ===\n",
            "Saving unified_schema.yaml...\n",
            "✓ Saved: unified_schema.yaml\n",
            "Saving rename_maps.json...\n",
            "✓ Saved: rename_maps.json\n",
            "Generating merge_report.md...\n",
            "Created missing 'is_candidate' column based on label values\n",
            "✓ Saved: merge_report.md\n",
            "Saving unified_exoplanets.parquet...\n",
            "✓ Saved: unified_exoplanets.parquet (21271 rows)\n",
            "Saving unified_exoplanets.csv...\n",
            "✓ Saved: unified_exoplanets.csv\n",
            "Saving unified_exoplanets_train.csv...\n",
            "✓ Saved: unified_exoplanets_train.csv (12194 rows with confirmed labels)\n",
            "\n",
            "================================================================================\n",
            "PROFESYONEL PIPELINE COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Generated artifacts:\n",
            "  1. unified_schema.yaml - Feature schema with dtypes and units\n",
            "  2. rename_maps.json - Column mapping dictionaries\n",
            "  3. merge_report.md - Data quality report\n",
            "  4. unified_exoplanets.parquet - Full unified dataset (Parquet)\n",
            "  5. unified_exoplanets.csv - Full unified dataset (CSV)\n",
            "  6. unified_exoplanets_train.csv - Training subset (confirmed labels only)\n",
            "\n",
            "Total unified records: 21,271\n",
            "Training records: 12,194\n",
            "Confirmed planets: 5,745\n",
            "False positives: 6,449\n",
            "Total features: 34\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=== PROFESSIONAL ARTIFACT EXPORT ===\")\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. Save unified schema as YAML\n",
        "print(\"Saving unified_schema.yaml...\")\n",
        "with open('unified_schema.yaml', 'w') as f:\n",
        "    yaml.dump(unified_schema, f, default_flow_style=False, sort_keys=False)\n",
        "print(\"✓ Saved: unified_schema.yaml\")\n",
        "\n",
        "# 2. Save rename maps as JSON\n",
        "print(\"Saving rename_maps.json...\")\n",
        "with open('rename_maps.json', 'w') as f:\n",
        "    json.dump(rename_maps, f, indent=2)\n",
        "print(\"✓ Saved: rename_maps.json\")\n",
        "\n",
        "# 3. Generate comprehensive data quality report\n",
        "print(\"Generating merge_report.md...\")\n",
        "report = []\n",
        "report.append(\"# NASA Exoplanet Data Merge Report\")\n",
        "report.append(f\"\\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "report.append(\"\\n---\\n\")\n",
        "\n",
        "# Summary statistics\n",
        "report.append(\"## Dataset Summary\\n\")\n",
        "report.append(f\"**Total records:** {len(unified_df):,}\")\n",
        "report.append(f\"**Total features:** {len(unified_df.columns)}\")\n",
        "report.append(f\"\\n**Source distribution:**\")\n",
        "for source, count in unified_df['source_dataset'].value_counts().items():\n",
        "    pct = count / len(unified_df) * 100\n",
        "    report.append(f\"- {source}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Label distribution\n",
        "report.append(\"\\n## Label Distribution\\n\")\n",
        "label_map = {1: 'Confirmed Planet', 0: 'False Positive', -1: 'Candidate'}\n",
        "for label_val, label_name in label_map.items():\n",
        "    count = (unified_df['label'] == label_val).sum()\n",
        "    pct = count / len(unified_df) * 100\n",
        "    report.append(f\"- **{label_name}**: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Check if is_candidate column exists, if not create it\n",
        "if 'is_candidate' not in unified_df.columns:\n",
        "    unified_df['is_candidate'] = (unified_df['label'] == -1).astype(int)\n",
        "    print(\"Created missing 'is_candidate' column based on label values\")\n",
        "\n",
        "candidates = unified_df['is_candidate'].sum()\n",
        "report.append(f\"\\n**Candidates flagged:** {candidates:,}\")\n",
        "\n",
        "# Data quality metrics\n",
        "report.append(\"\\n## Data Quality Metrics\\n\")\n",
        "\n",
        "# Completeness\n",
        "completeness = (1 - unified_df.isnull().sum() / len(unified_df)) * 100\n",
        "avg_completeness = completeness.mean()\n",
        "report.append(f\"**Average completeness:** {avg_completeness:.1f}%\\n\")\n",
        "\n",
        "report.append(\"**Completeness by feature (bottom 10):**\")\n",
        "bottom_10 = completeness.sort_values().head(10)\n",
        "for col, comp in bottom_10.items():\n",
        "    report.append(f\"- {col}: {comp:.1f}%\")\n",
        "\n",
        "# Key feature availability\n",
        "report.append(\"\\n## Key Feature Availability\\n\")\n",
        "key_features = ['period_days', 'rp_re', 'transit_depth_ppm', 'teff_k', 'duration_hours']\n",
        "for feat in key_features:\n",
        "    if feat in unified_df.columns:\n",
        "        avail = (unified_df[feat].notna().sum() / len(unified_df)) * 100\n",
        "        report.append(f\"- **{feat}**: {avail:.1f}% available\")\n",
        "\n",
        "# Cross-dataset comparisons\n",
        "report.append(\"\\n## Cross-Dataset Comparisons\\n\")\n",
        "for source in unified_df['source_dataset'].unique():\n",
        "    source_df = unified_df[unified_df['source_dataset'] == source]\n",
        "    report.append(f\"\\n### {source.upper()} Dataset\")\n",
        "    report.append(f\"- Records: {len(source_df):,}\")\n",
        "    report.append(f\"- Confirmed planets: {(source_df['label'] == 1).sum()}\")\n",
        "    report.append(f\"- False positives: {(source_df['label'] == 0).sum()}\")\n",
        "    report.append(f\"- Candidates: {(source_df['label'] == -1).sum()}\")\n",
        "\n",
        "# Recommendations\n",
        "report.append(\"\\n## Recommendations\\n\")\n",
        "report.append(\"1. **Training strategy**: Use only confirmed (label=1) and false positive (label=0) for supervised training\")\n",
        "report.append(\"2. **Candidate handling**: Treat candidates (label=-1) as unlabeled data for semi-supervised learning or exclude from training\")\n",
        "report.append(\"3. **Feature engineering**: Consider log-transforming skewed features (period_days, rp_re, etc.)\")\n",
        "report.append(\"4. **Imbalanced classes**: Apply SMOTE or class weighting if false positives significantly outnumber confirmed planets\")\n",
        "report.append(\"5. **Missing values**: Features with <50% completeness may need removal or careful imputation validation\")\n",
        "\n",
        "# Save report\n",
        "with open('merge_report.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(report))\n",
        "print(\"✓ Saved: merge_report.md\")\n",
        "\n",
        "# 4. Save unified dataset as Parquet (efficient format)\n",
        "print(\"Saving unified_exoplanets.parquet...\")\n",
        "unified_df.to_parquet('unified_exoplanets.parquet', index=False)\n",
        "print(f\"✓ Saved: unified_exoplanets.parquet ({unified_df.shape[0]} rows)\")\n",
        "\n",
        "# 5. Save unified dataset as CSV\n",
        "print(\"Saving unified_exoplanets.csv...\")\n",
        "unified_df.to_csv('unified_exoplanets.csv', index=False)\n",
        "print(f\"✓ Saved: unified_exoplanets.csv\")\n",
        "\n",
        "# 6. Save training dataset (confirmed labels only)\n",
        "print(\"Saving unified_exoplanets_train.csv...\")\n",
        "train_df = unified_df[unified_df['label'].isin([0, 1])].copy()\n",
        "train_df.to_csv('unified_exoplanets_train.csv', index=False)\n",
        "print(f\"✓ Saved: unified_exoplanets_train.csv ({len(train_df)} rows with confirmed labels)\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROFESSIONAL PIPELINE COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nGenerated artifacts:\")\n",
        "print(f\"  1. unified_schema.yaml - Feature schema with dtypes and units\")\n",
        "print(f\"  2. rename_maps.json - Column mapping dictionaries\")\n",
        "print(f\"  3. merge_report.md - Data quality report\")\n",
        "print(f\"  4. unified_exoplanets.parquet - Full unified dataset (Parquet)\")\n",
        "print(f\"  5. unified_exoplanets.csv - Full unified dataset (CSV)\")\n",
        "print(f\"  6. unified_exoplanets_train.csv - Training subset (confirmed labels only)\")\n",
        "print(f\"\\nTotal unified records: {len(unified_df):,}\")\n",
        "print(f\"Training records: {len(train_df):,}\")\n",
        "print(f\"Confirmed planets: {(train_df['label'] == 1).sum():,}\")\n",
        "print(f\"False positives: {(train_df['label'] == 0).sum():,}\")\n",
        "print(f\"Total features: {len(unified_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SMART FEATURE SELECTION ===\n",
            "\n",
            "1. TRAINING DATA HAZIRLIĞI\n",
            "Training data: (12194, 34)\n",
            "Confirmed planets: 5745\n",
            "False positives: 6449\n",
            "\n",
            "2. CANDIDATE DATA HAZIRLIĞI\n",
            "Candidate data: (9077, 34)\n",
            "Candidates to predict: 9077\n",
            "\n",
            "3. SMART FEATURE SELECTION\n",
            "All numeric features: 29\n",
            "\n",
            "4. FEATURE CORRELATION ANALYSIS\n",
            "High correlation pairs (>0.95): 1\n",
            "Top 5 high correlation pairs:\n",
            "  impact_param <-> ror_ratio: 0.979\n",
            "Removing 1 highly correlated features\n",
            "\n",
            "5. FEATURE IMPORTANCE ANALYSIS\n",
            "Selected 20 most important features\n",
            "Top 10 features: ['fp_flag_co', 'fp_flag_ss', 'fp_flag_nt', 'rp_re', 'dor_ratio', 't0_bjd', 'num_transits', 'snr', 'radius_solar', 'teq_k']\n",
            "\n",
            "6. FINAL FEATURE PREPARATION\n",
            "Final numeric features: 20\n",
            "Training X shape: (12194, 20)\n",
            "Training y shape: (12194,)\n",
            "Candidate X shape: (9077, 20)\n",
            "\n",
            "✅ Smart feature selection complete!\n",
            "Next: XGBoost model training and candidate prediction...\n"
          ]
        }
      ],
      "source": [
        "# SMART FEATURE SELECTION\n",
        "print(\"=== SMART FEATURE SELECTION ===\")\n",
        "\n",
        "# 1. TRAINING DATA: Sadece confirmed + false positive\n",
        "print(\"\\n1. TRAINING DATA HAZIRLIĞI\")\n",
        "train_data = unified_df[unified_df['label'].isin([0, 1])].copy()\n",
        "print(f\"Training data: {train_data.shape}\")\n",
        "print(f\"Confirmed planets: {(train_data['label'] == 1).sum()}\")\n",
        "print(f\"False positives: {(train_data['label'] == 0).sum()}\")\n",
        "\n",
        "# 2. CANDIDATE DATA: Tahmin edilecek veriler\n",
        "print(\"\\n2. CANDIDATE DATA HAZIRLIĞI\")\n",
        "candidate_data = unified_df[unified_df['label'] == -1].copy()\n",
        "print(f\"Candidate data: {candidate_data.shape}\")\n",
        "print(f\"Candidates to predict: {len(candidate_data)}\")\n",
        "\n",
        "# 3. SMART FEATURE SELECTION\n",
        "print(\"\\n3. SMART FEATURE SELECTION\")\n",
        "# Get all numeric features\n",
        "all_numeric_features = [col for col in unified_df.columns \n",
        "                       if col not in ['star_id', 'planet_id', 'source_dataset', 'label', 'is_candidate']\n",
        "                       and unified_df[col].dtype in ['float64', 'int64', 'int8', 'int16']]\n",
        "\n",
        "print(f\"All numeric features: {len(all_numeric_features)}\")\n",
        "\n",
        "# Feature correlation analysis\n",
        "print(\"\\n4. FEATURE CORRELATION ANALYSIS\")\n",
        "correlation_matrix = train_data[all_numeric_features].corr().abs()\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        if correlation_matrix.iloc[i, j] > 0.95:  # Very high correlation\n",
        "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
        "\n",
        "print(f\"High correlation pairs (>0.95): {len(high_corr_pairs)}\")\n",
        "if len(high_corr_pairs) > 0:\n",
        "    print(\"Top 5 high correlation pairs:\")\n",
        "    for pair in high_corr_pairs[:5]:\n",
        "        print(f\"  {pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "# Remove highly correlated features\n",
        "features_to_remove = set()\n",
        "for pair in high_corr_pairs:\n",
        "    # Keep the feature with higher variance\n",
        "    var1 = train_data[pair[0]].var()\n",
        "    var2 = train_data[pair[1]].var()\n",
        "    if var1 > var2:\n",
        "        features_to_remove.add(pair[1])\n",
        "    else:\n",
        "        features_to_remove.add(pair[0])\n",
        "\n",
        "print(f\"Removing {len(features_to_remove)} highly correlated features\")\n",
        "selected_features = [col for col in all_numeric_features if col not in features_to_remove]\n",
        "\n",
        "# 5. FEATURE IMPORTANCE WITH QUICK RANDOM FOREST\n",
        "print(\"\\n5. FEATURE IMPORTANCE ANALYSIS\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Quick feature importance analysis\n",
        "rf_selector = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "X_temp = train_data[selected_features].fillna(train_data[selected_features].median())\n",
        "y_temp = train_data['label']\n",
        "\n",
        "rf_selector.fit(X_temp, y_temp)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': selected_features,\n",
        "    'importance': rf_selector.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Select top features (keep top 70% or minimum 20 features)\n",
        "n_features = max(20, int(len(selected_features) * 0.7))\n",
        "top_features = feature_importance.head(n_features)['feature'].tolist()\n",
        "\n",
        "print(f\"Selected {len(top_features)} most important features\")\n",
        "print(f\"Top 10 features: {top_features[:10]}\")\n",
        "\n",
        "# 6. FINAL FEATURE PREPARATION\n",
        "print(\"\\n6. FINAL FEATURE PREPARATION\")\n",
        "numeric_features = top_features\n",
        "\n",
        "print(f\"Final numeric features: {len(numeric_features)}\")\n",
        "\n",
        "# Prepare training features\n",
        "X_train = train_data[numeric_features].fillna(train_data[numeric_features].median())\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Prepare candidate features\n",
        "X_candidates = candidate_data[numeric_features].fillna(candidate_data[numeric_features].median())\n",
        "\n",
        "print(f\"Training X shape: {X_train.shape}\")\n",
        "print(f\"Training y shape: {y_train.shape}\")\n",
        "print(f\"Candidate X shape: {X_candidates.shape}\")\n",
        "\n",
        "print(\"\\n✅ Smart feature selection complete!\")\n",
        "print(\"Next: XGBoost model training and candidate prediction...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VERİ KAYDETME ===\n",
            "\n",
            "1. Training data kaydediliyor...\n",
            "✅ Training data kaydedildi: (12194, 34)\n",
            "\n",
            "2. Candidate data kaydediliyor...\n",
            "✅ Candidate data kaydedildi: (9077, 34)\n",
            "\n",
            "3. Feature matrices kaydediliyor...\n",
            "✅ Training features kaydedildi: (12194, 21)\n",
            "✅ Candidate features kaydedildi: (9077, 20)\n",
            "\n",
            "4. Unified dataset kaydediliyor...\n",
            "✅ Unified dataset kaydedildi: (21271, 34)\n",
            "\n",
            "✅ Tüm veriler CSV formatında kaydedildi!\n",
            "Dosyalar:\n",
            "  - training_data.csv\n",
            "  - candidate_data.csv\n",
            "  - X_train_features.csv\n",
            "  - X_candidates_features.csv\n",
            "  - unified_dataset_final.csv\n"
          ]
        }
      ],
      "source": [
        "# DATA SAVE - CSV FORMAT\n",
        "print(\"=== DATA SAVE ===\")\n",
        "\n",
        "# 1. Save training data\n",
        "print(\"\\n1. Saving training data...\")\n",
        "train_data.to_csv('training_data.csv', index=False)\n",
        "print(f\"✅ Training data saved: {train_data.shape}\")\n",
        "\n",
        "# 2. Save candidate data\n",
        "print(\"\\n2. Saving candidate data...\")\n",
        "candidate_data.to_csv('candidate_data.csv', index=False)\n",
        "print(f\"✅ Candidate data saved: {candidate_data.shape}\")\n",
        "\n",
        "# 3. Save feature matrices\n",
        "print(\"\\n3. Saving feature matrices...\")\n",
        "# Save X_train and y_train as a DataFrame\n",
        "train_features_df = pd.DataFrame(X_train, columns=numeric_features)\n",
        "train_features_df['label'] = y_train\n",
        "train_features_df.to_csv('X_train_features.csv', index=False)\n",
        "print(f\"✅ Training features saved: {train_features_df.shape}\")\n",
        "\n",
        "# Save X_candidates\n",
        "candidate_features_df = pd.DataFrame(X_candidates, columns=numeric_features)\n",
        "candidate_features_df.to_csv('X_candidates_features.csv', index=False)\n",
        "print(f\"✅ Candidate features saved: {candidate_features_df.shape}\")\n",
        "\n",
        "# 4. Save unified dataset (updated)\n",
        "print(\"\\n4. Saving unified dataset...\")\n",
        "unified_df.to_csv('unified_dataset_final.csv', index=False)\n",
        "print(f\"✅ Unified dataset saved: {unified_df.shape}\")\n",
        "\n",
        "print(\"\\n✅ All data saved in CSV format!\")\n",
        "print(\"Files:\")\n",
        "print(\"  - training_data.csv\")\n",
        "print(\"  - candidate_data.csv\") \n",
        "print(\"  - X_train_features.csv\")\n",
        "print(\"  - X_candidates_features.csv\")\n",
        "print(\"  - unified_dataset_final.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NasaExoplanet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
